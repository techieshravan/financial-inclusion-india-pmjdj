{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa30d505-1997-4b82-8b8f-7538f7a92ddb",
   "metadata": {},
   "source": [
    "### Machine Learning Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d82d198-1fc0-4589-ad0e-af07f4f5237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc23715-0deb-4338-adf4-75594ad74c29",
   "metadata": {},
   "source": [
    "### Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc14b5c-3f72-4e16-9491-bed6119c79f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREPROCESSING STEPS FOR PMJDY ML ANALYSIS:\n",
      "\n",
      "1. DATA LOADING & INITIAL INSPECTION\n",
      "   - Load the cleaned datasets\n",
      "   - Check data types and missing values\n",
      "   - Identify feature categories\n",
      "\n",
      "2. HANDLING MISSING VALUES\n",
      "   - Impute missing census data\n",
      "   - Handle missing population values\n",
      "   - Strategy: median imputation for numerical features\n",
      "\n",
      "3. FEATURE SCALING & NORMALIZATION\n",
      "   - Standardize continuous features\n",
      "   - Keep binary flags as-is\n",
      "   - Separate scaling for different feature groups\n",
      "\n",
      "4. HANDLING CLASS IMBALANCE\n",
      "   - Check target variable distributions\n",
      "   - Apply SMOTE for imbalanced flags\n",
      "   - Create balanced training sets\n",
      "\n",
      "5. FEATURE ENGINEERING\n",
      "   - Create interaction features\n",
      "   - Generate polynomial features for key metrics\n",
      "   - Add regional clustering features\n",
      "\n",
      "6. DATA SPLITTING\n",
      "   - Train-test split (80-20)\n",
      "   - Stratified sampling for classification\n",
      "   - Time-based split for temporal analysis\n",
      "\n",
      "7. FINAL VALIDATION\n",
      "   - Check for data leakage\n",
      "   - Verify feature distributions\n",
      "   - Export preprocessed datasets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessing_steps = \"\"\"\n",
    "PREPROCESSING STEPS FOR PMJDY ML ANALYSIS:\n",
    "\n",
    "1. DATA LOADING & INITIAL INSPECTION\n",
    "   - Load the cleaned datasets\n",
    "   - Check data types and missing values\n",
    "   - Identify feature categories\n",
    "\n",
    "2. HANDLING MISSING VALUES\n",
    "   - Impute missing census data\n",
    "   - Handle missing population values\n",
    "   - Strategy: median imputation for numerical features\n",
    "\n",
    "3. FEATURE SCALING & NORMALIZATION\n",
    "   - Standardize continuous features\n",
    "   - Keep binary flags as-is\n",
    "   - Separate scaling for different feature groups\n",
    "\n",
    "4. HANDLING CLASS IMBALANCE\n",
    "   - Check target variable distributions\n",
    "   - Apply SMOTE for imbalanced flags\n",
    "   - Create balanced training sets\n",
    "\n",
    "5. FEATURE ENGINEERING\n",
    "   - Create interaction features\n",
    "   - Generate polynomial features for key metrics\n",
    "   - Add regional clustering features\n",
    "\n",
    "6. DATA SPLITTING\n",
    "   - Train-test split (80-20)\n",
    "   - Stratified sampling for classification\n",
    "   - Time-based split for temporal analysis\n",
    "\n",
    "7. FINAL VALIDATION\n",
    "   - Check for data leakage\n",
    "   - Verify feature distributions\n",
    "   - Export preprocessed datasets\n",
    "\"\"\"\n",
    "\n",
    "print(preprocessing_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e81821-6ccb-426e-a33e-3d88a527ddb2",
   "metadata": {},
   "source": [
    "### STEP 1: DATA LOADING & INITIAL INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4a16db-676a-4734-a7c5-f42984c353a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: DATA LOADING & INITIAL INSPECTION\n",
      "\n",
      "Executing Step 1...\n",
      "\n",
      "Loading datasets...\n",
      "Loaded ml_integrated_dataset: (36, 52)\n",
      "\n",
      "Column categories:\n",
      "  - Categorical: 2 columns\n",
      "  - Continuous: 45 columns\n",
      "  - Binary flags: 5 columns\n",
      "\n",
      "Missing values found in 16 columns:\n",
      "  - Rural_Beneficiaries: 2 (5.6%)\n",
      "  - Urban_Beneficiaries: 2 (5.6%)\n",
      "  - Total_Beneficiaries: 2 (5.6%)\n",
      "  - Balance_Rupees: 2 (5.6%)\n",
      "  - RuPay_Cards: 2 (5.6%)\n",
      "  - Rural_Percent: 2 (5.6%)\n",
      "  - RuPay_Penetration: 2 (5.6%)\n",
      "  - Avg_Balance_Rs: 2 (5.6%)\n",
      "  - Rural_Urban_Ratio: 2 (5.6%)\n",
      "  - Population: 7 (19.4%)\n",
      "  - Households: 7 (19.4%)\n",
      "  - Literacy_Rate: 7 (19.4%)\n",
      "  - Rural_HH_Percent: 7 (19.4%)\n",
      "  - Internet_Penetration: 7 (19.4%)\n",
      "  - Phone_Penetration: 7 (19.4%)\n",
      "  - Account_Density_Per_Lakh: 7 (19.4%)\n",
      "\n",
      "Step 1 Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 1: DATA LOADING & INITIAL INSPECTION\")\n",
    "\n",
    "def load_and_inspect_data():\n",
    "    \"\"\"Load cleaned datasets and perform initial inspection\"\"\"\n",
    "    \n",
    "    print(\"\\nLoading datasets...\")\n",
    "    \n",
    "    # Load the ml_integrated_dataset as primary dataset\n",
    "    ml_data = pd.read_csv('./cleaned datasets/ml_integrated_dataset.csv')\n",
    "    print(f\"Loaded ml_integrated_dataset: {ml_data.shape}\")\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_cols = ml_data.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = ml_data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    binary_flags = [col for col in numerical_cols if 'Flag' in col]\n",
    "    continuous_features = [col for col in numerical_cols if col not in binary_flags]\n",
    "    \n",
    "    print(f\"\\nColumn categories:\")\n",
    "    print(f\"  - Categorical: {len(categorical_cols)} columns\")\n",
    "    print(f\"  - Continuous: {len(continuous_features)} columns\")\n",
    "    print(f\"  - Binary flags: {len(binary_flags)} columns\")\n",
    "    \n",
    "    # Check missing values\n",
    "    missing_summary = ml_data.isnull().sum()\n",
    "    missing_cols = missing_summary[missing_summary > 0]\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"\\nMissing values found in {len(missing_cols)} columns:\")\n",
    "        for col, count in missing_cols.items():\n",
    "            print(f\"  - {col}: {count} ({count/len(ml_data)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found\")\n",
    "    \n",
    "    return ml_data, categorical_cols, continuous_features, binary_flags\n",
    "\n",
    "# Execute Step 1\n",
    "print(\"\\nExecuting Step 1...\")\n",
    "\n",
    "ml_data, categorical_cols, continuous_features, binary_flags = load_and_inspect_data()\n",
    "\n",
    "print(\"\\nStep 1 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5afef-ce24-4ed8-a02b-19c864fe166a",
   "metadata": {},
   "source": [
    "### STEP 2: HANDLING MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b1eb8b-9346-47ec-9476-c075aa1234cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: HANDLING MISSING VALUES\n",
      "\n",
      "Executing Step 2...\n",
      "\n",
      "Imputing missing values in 16 columns...\n",
      "Imputation strategy: MEDIAN (robust to outliers)\n",
      "\n",
      "Imputed values:\n",
      "  - Rural_Beneficiaries: 4808972.50\n",
      "  - Urban_Beneficiaries: 3763875.00\n",
      "  - Total_Beneficiaries: 8289702.00\n",
      "  - Balance_Rupees: 39484500000.00\n",
      "  - RuPay_Cards: 6048789.00\n",
      "  - Rural_Percent: 65.65\n",
      "  - RuPay_Penetration: 68.00\n",
      "  - Avg_Balance_Rs: 5161.10\n",
      "  - Rural_Urban_Ratio: 1.92\n",
      "  - Population: 25545198.00\n",
      "  - Households: 7088008.00\n",
      "  - Literacy_Rate: 67.43\n",
      "  - Rural_HH_Percent: 67.10\n",
      "  - Internet_Penetration: 1.78\n",
      "  - Phone_Penetration: 47.93\n",
      "  - Account_Density_Per_Lakh: 33918.95\n",
      "\n",
      "Remaining missing values: 0\n",
      "\n",
      "Step 2 Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: HANDLING MISSING VALUES\")\n",
    "\n",
    "def handle_missing_values(df, continuous_features):\n",
    "    \"\"\"Impute missing values using appropriate strategies\"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Identify columns with missing values\n",
    "    missing_cols = df_processed[continuous_features].columns[df_processed[continuous_features].isnull().any()].tolist()\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"\\nImputing missing values in {len(missing_cols)} columns...\")\n",
    "        \n",
    "        # Use median imputation for continuous features\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df_processed[missing_cols] = imputer.fit_transform(df_processed[missing_cols])\n",
    "        \n",
    "        print(\"Imputation strategy: MEDIAN (robust to outliers)\")\n",
    "        print(\"\\nImputed values:\")\n",
    "        for col in missing_cols:\n",
    "            median_val = imputer.statistics_[missing_cols.index(col)]\n",
    "            print(f\"  - {col}: {median_val:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values to impute\")\n",
    "    \n",
    "    # Verify no missing values remain\n",
    "    remaining_missing = df_processed[continuous_features].isnull().sum().sum()\n",
    "    print(f\"\\nRemaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Execute Step 2\n",
    "print(\"\\nExecuting Step 2...\")\n",
    "\n",
    "ml_data_imputed = handle_missing_values(ml_data, continuous_features)\n",
    "\n",
    "print(\"\\nStep 2 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba996e55-062d-450e-bbc4-baafdb17e79c",
   "metadata": {},
   "source": [
    "### STEP 3: FEATURE SCALING & NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134a4836-14e7-4826-83d7-a406fade257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: FEATURE SCALING & NORMALIZATION\n",
      "\n",
      "Executing Step 3...\n",
      "\n",
      "Feature groups identified:\n",
      "  - Growth metrics: 7 features\n",
      "  - Rate/Percentage metrics: 12 features\n",
      "  - Absolute values: 26 features\n",
      "\n",
      "Applying StandardScaler to growth features...\n",
      "Applying MinMaxScaler to rate features...\n",
      "Applying RobustScaler to absolute features...\n",
      "\n",
      "Scaling complete for 45 continuous features\n",
      "Binary flags preserved as-is: 5 features\n",
      "\n",
      "Step 3 Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 3: FEATURE SCALING & NORMALIZATION\")\n",
    "\n",
    "def scale_features(df, continuous_features, binary_flags):\n",
    "    \"\"\"Apply appropriate scaling to different feature groups\"\"\"\n",
    "    \n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Separate features by their characteristics\n",
    "    growth_features = [col for col in continuous_features if 'Growth' in col or 'CAGR' in col]\n",
    "    rate_features = [col for col in continuous_features if 'Rate' in col or 'Percent' in col or 'Penetration' in col]\n",
    "    absolute_features = [col for col in continuous_features if col not in growth_features + rate_features]\n",
    "    \n",
    "    print(f\"\\nFeature groups identified:\")\n",
    "    print(f\"  - Growth metrics: {len(growth_features)} features\")\n",
    "    print(f\"  - Rate/Percentage metrics: {len(rate_features)} features\")\n",
    "    print(f\"  - Absolute values: {len(absolute_features)} features\")\n",
    "    \n",
    "    # StandardScaler for growth features (can be negative)\n",
    "    if growth_features:\n",
    "        print(\"\\nApplying StandardScaler to growth features...\")\n",
    "        scaler_growth = StandardScaler()\n",
    "        df_scaled[growth_features] = scaler_growth.fit_transform(df[growth_features])\n",
    "    \n",
    "    # MinMaxScaler for rate features (bounded 0-100)\n",
    "    if rate_features:\n",
    "        print(\"Applying MinMaxScaler to rate features...\")\n",
    "        scaler_rate = MinMaxScaler()\n",
    "        df_scaled[rate_features] = scaler_rate.fit_transform(df[rate_features])\n",
    "    \n",
    "    # RobustScaler for absolute features (handles outliers)\n",
    "    if absolute_features:\n",
    "        print(\"Applying RobustScaler to absolute features...\")\n",
    "        scaler_absolute = RobustScaler()\n",
    "        df_scaled[absolute_features] = scaler_absolute.fit_transform(df[absolute_features])\n",
    "    \n",
    "    print(f\"\\nScaling complete for {len(continuous_features)} continuous features\")\n",
    "    print(f\"Binary flags preserved as-is: {len(binary_flags)} features\")\n",
    "    \n",
    "    return df_scaled, scaler_growth, scaler_rate, scaler_absolute\n",
    "\n",
    "# Execute Step 3\n",
    "print(\"\\nExecuting Step 3...\")\n",
    "\n",
    "ml_data_scaled, scaler_g, scaler_r, scaler_a = scale_features(ml_data_imputed, continuous_features, binary_flags)\n",
    "\n",
    "print(\"\\nStep 3 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13c9c1-1535-4a1b-b838-c974f411ea9b",
   "metadata": {},
   "source": [
    "### STEP 4: HANDLING CLASS IMBALANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c80f1f-bf48-4ac7-b9af-65a8419079cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: HANDLING CLASS IMBALANCE\n",
      "\n",
      "Executing Step 4...\n",
      "\n",
      "Target variable distributions:\n",
      "----------------------------------------\n",
      "\n",
      "High_Operative_Flag:\n",
      "  Class 0: 11 samples\n",
      "  Class 1: 25 samples\n",
      "  Balance ratio: 0.44\n",
      "Balanced\n",
      "\n",
      "High_Growth_Flag:\n",
      "  Class 0: 36 samples\n",
      "  Class 1: 0 samples\n",
      "  Balance ratio: 1.00\n",
      "Balanced\n",
      "\n",
      "High_RuPay_Flag:\n",
      "  Class 0: 22 samples\n",
      "  Class 1: 14 samples\n",
      "  Balance ratio: 0.64\n",
      "Balanced\n",
      "\n",
      "High_Balance_Flag:\n",
      "  Class 0: 9 samples\n",
      "  Class 1: 27 samples\n",
      "  Balance ratio: 0.33\n",
      "Balanced\n",
      "\n",
      "Rural_Dominated_Flag:\n",
      "  Class 0: 22 samples\n",
      "  Class 1: 14 samples\n",
      "  Balance ratio: 0.64\n",
      "Balanced\n",
      "\n",
      "0 targets need balancing: []\n",
      "\n",
      "Step 4 Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 4: HANDLING CLASS IMBALANCE\")\n",
    "\n",
    "def check_and_balance_targets(df, binary_flags):\n",
    "    \"\"\"Check target distributions and apply SMOTE if needed\"\"\"\n",
    "    \n",
    "    print(\"\\nTarget variable distributions:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    imbalanced_targets = []\n",
    "    \n",
    "    for flag in binary_flags:\n",
    "        counts = df[flag].value_counts()\n",
    "        ratio = counts.min() / counts.max() if len(counts) == 2 else 1\n",
    "        \n",
    "        print(f\"\\n{flag}:\")\n",
    "        print(f\"  Class 0: {counts.get(0, 0)} samples\")\n",
    "        print(f\"  Class 1: {counts.get(1, 0)} samples\")\n",
    "        print(f\"  Balance ratio: {ratio:.2f}\")\n",
    "        \n",
    "        if ratio < 0.3:  # Significant imbalance\n",
    "            imbalanced_targets.append(flag)\n",
    "            print(f\"IMBALANCED - needs balancing\")\n",
    "        else:\n",
    "            print(f\"Balanced\")\n",
    "    \n",
    "    print(f\"\\n{len(imbalanced_targets)} targets need balancing: {imbalanced_targets}\")\n",
    "    \n",
    "    # Prepare balanced datasets for each imbalanced target\n",
    "    balanced_datasets = {}\n",
    "    \n",
    "    for target in imbalanced_targets:\n",
    "        print(f\"\\nBalancing {target} using SMOTE...\")\n",
    "        \n",
    "        # Prepare features and target\n",
    "        feature_cols = [col for col in df.columns if col not in binary_flags + ['State/UT']]\n",
    "        X = df[feature_cols]\n",
    "        y = df[target]\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "        \n",
    "        balanced_datasets[target] = {\n",
    "            'X': X_balanced,\n",
    "            'y': y_balanced,\n",
    "            'original_ratio': y.value_counts()[1] / len(y),\n",
    "            'balanced_ratio': y_balanced.value_counts()[1] / len(y_balanced)\n",
    "        }\n",
    "        \n",
    "        print(f\"  Original minority class: {y.value_counts()[1]} ({balanced_datasets[target]['original_ratio']:.1%})\")\n",
    "        print(f\"  Balanced minority class: {y_balanced.value_counts()[1]} ({balanced_datasets[target]['balanced_ratio']:.1%})\")\n",
    "    \n",
    "    return balanced_datasets, imbalanced_targets\n",
    "\n",
    "# Execute Step 4\n",
    "print(\"\\nExecuting Step 4...\")\n",
    "\n",
    "balanced_data, imbalanced_flags = check_and_balance_targets(ml_data_scaled, binary_flags)\n",
    "\n",
    "print(\"\\nStep 4 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858de645-83d2-42ed-8184-c595ea02498d",
   "metadata": {},
   "source": [
    "### STEP 5: FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def1f650-8e35-4bb6-ad22-3ba23a019c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: FEATURE ENGINEERING\n",
      "\n",
      "Executing Step 5...\n",
      "\n",
      "Creating interaction features...\n",
      "  Growth_Operative_Interaction\n",
      "  Account_Density_Squared\n",
      "\n",
      "Creating regional clusters...\n",
      "  Regional dummies created: ['Region_Central', 'Region_East', 'Region_North', 'Region_Northeast', 'Region_South', 'Region_West']\n",
      "\n",
      "Feature engineering complete\n",
      "  Original features: 52\n",
      "  Engineered features: 61\n",
      "  New features added: 9\n",
      "\n",
      "Step 5 Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 5: FEATURE ENGINEERING\")\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create interaction and polynomial features\"\"\"\n",
    "    \n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    print(\"\\nCreating interaction features...\")\n",
    "    \n",
    "    # 1. Rural-Urban interaction features\n",
    "    if 'Rural_Percent' in df.columns and 'Jul25_RuPay_Penetration' in df.columns:\n",
    "        df_engineered['Rural_RuPay_Interaction'] = df['Rural_Percent'] * df['Jul25_RuPay_Penetration'] / 100\n",
    "        print(\"  Rural_RuPay_Interaction\")\n",
    "    \n",
    "    # 2. Growth-Operative interaction\n",
    "    if 'Growth_2024_25' in df.columns and 'Jan25_Op_Rate' in df.columns:\n",
    "        df_engineered['Growth_Operative_Interaction'] = df['Growth_2024_25'] * df['Jan25_Op_Rate'] / 100\n",
    "        print(\"  Growth_Operative_Interaction\")\n",
    "    \n",
    "    # 3. Account density squared (non-linear relationship)\n",
    "    if 'Account_Density_Per_Lakh' in df.columns:\n",
    "        df_engineered['Account_Density_Squared'] = df['Account_Density_Per_Lakh'] ** 2\n",
    "        print(\"  Account_Density_Squared\")\n",
    "    \n",
    "    # 4. Balance-Operative efficiency\n",
    "    if 'Jul25_Avg_Balance' in df.columns and 'Jan25_Op_Rate' in df.columns:\n",
    "        df_engineered['Balance_Efficiency'] = df['Jul25_Avg_Balance'] * df['Jan25_Op_Rate'] / 100\n",
    "        print(\"  Balance_Efficiency\")\n",
    "    \n",
    "    # 5. Regional clusters based on performance\n",
    "    print(\"\\nCreating regional clusters...\")\n",
    "    \n",
    "    # Define regions based on geographic proximity\n",
    "    northern_states = ['Delhi', 'Haryana', 'Himachal Pradesh', 'Jammu And Kashmir', \n",
    "                      'Punjab', 'Rajasthan', 'Uttarakhand', 'Chandigarh']\n",
    "    southern_states = ['Andhra Pradesh', 'Karnataka', 'Kerala', 'Tamil Nadu', \n",
    "                      'Telangana', 'Puducherry', 'Lakshadweep']\n",
    "    eastern_states = ['Bihar', 'Jharkhand', 'Odisha', 'West Bengal', 'Sikkim']\n",
    "    western_states = ['Goa', 'Gujarat', 'Maharashtra', 'Dadra And Nagar Haveli And Daman And Diu']\n",
    "    northeastern_states = ['Arunachal Pradesh', 'Assam', 'Manipur', 'Meghalaya', \n",
    "                          'Mizoram', 'Nagaland', 'Tripura']\n",
    "    \n",
    "    def assign_region(state):\n",
    "        if state in northern_states:\n",
    "            return 'North'\n",
    "        elif state in southern_states:\n",
    "            return 'South'\n",
    "        elif state in eastern_states:\n",
    "            return 'East'\n",
    "        elif state in western_states:\n",
    "            return 'West'\n",
    "        elif state in northeastern_states:\n",
    "            return 'Northeast'\n",
    "        else:\n",
    "            return 'Central'\n",
    "    \n",
    "    if 'State/UT' in df.columns:\n",
    "        df_engineered['Region'] = df['State/UT'].apply(assign_region)\n",
    "        \n",
    "        # Create dummy variables for regions\n",
    "        region_dummies = pd.get_dummies(df_engineered['Region'], prefix='Region')\n",
    "        df_engineered = pd.concat([df_engineered, region_dummies], axis=1)\n",
    "        print(f\"  Regional dummies created: {region_dummies.columns.tolist()}\")\n",
    "    \n",
    "    print(f\"\\nFeature engineering complete\")\n",
    "    print(f\"  Original features: {df.shape[1]}\")\n",
    "    print(f\"  Engineered features: {df_engineered.shape[1]}\")\n",
    "    print(f\"  New features added: {df_engineered.shape[1] - df.shape[1]}\")\n",
    "    \n",
    "    return df_engineered\n",
    "\n",
    "# Execute Step 5\n",
    "print(\"\\nExecuting Step 5...\")\n",
    "\n",
    "ml_data_engineered = engineer_features(ml_data_scaled)\n",
    "\n",
    "print(\"\\nStep 5 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff5da1-5506-48d1-bbdf-7d1236240f00",
   "metadata": {},
   "source": [
    "### STEP 6: DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da866b0a-6bdb-4b51-bfb3-8ccaa8eff2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: DATA SPLITTING\n",
      "\n",
      "Executing Step 6...\n",
      "\n",
      "Features for ML: 54 columns\n",
      "\n",
      "Splitting data for High_Operative_Flag...\n",
      "  Train: 28 samples (Positive: 67.9%)\n",
      "  Test:  8 samples (Positive: 75.0%)\n",
      "\n",
      "Splitting data for High_Growth_Flag...\n",
      "  Train: 28 samples (Positive: 0.0%)\n",
      "  Test:  8 samples (Positive: 0.0%)\n",
      "\n",
      "Splitting data for High_RuPay_Flag...\n",
      "  Train: 28 samples (Positive: 39.3%)\n",
      "  Test:  8 samples (Positive: 37.5%)\n",
      "\n",
      "Splitting data for High_Balance_Flag...\n",
      "  Train: 28 samples (Positive: 75.0%)\n",
      "  Test:  8 samples (Positive: 75.0%)\n",
      "\n",
      "Splitting data for Rural_Dominated_Flag...\n",
      "  Train: 28 samples (Positive: 39.3%)\n",
      "  Test:  8 samples (Positive: 37.5%)\n",
      "\n",
      "Step 6 Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 6: DATA SPLITTING\")\n",
    "\n",
    "def split_data_for_ml(df, binary_flags):\n",
    "    \"\"\"Create train-test splits for different ML tasks\"\"\"\n",
    "    \n",
    "    splits = {}\n",
    "    \n",
    "    # Prepare feature columns (exclude state name and target variables)\n",
    "    feature_cols = [col for col in df.columns if col not in binary_flags + ['State/UT', 'Region']]\n",
    "    \n",
    "    print(f\"\\nFeatures for ML: {len(feature_cols)} columns\")\n",
    "    \n",
    "    for target in binary_flags:\n",
    "        print(f\"\\nSplitting data for {target}...\")\n",
    "        \n",
    "        X = df[feature_cols]\n",
    "        y = df[target]\n",
    "        \n",
    "        # Stratified split to maintain class distribution\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        splits[target] = {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'train_positive_ratio': y_train.mean(),\n",
    "            'test_positive_ratio': y_test.mean()\n",
    "        }\n",
    "        \n",
    "        print(f\"  Train: {splits[target]['train_size']} samples (Positive: {splits[target]['train_positive_ratio']:.1%})\")\n",
    "        print(f\"  Test:  {splits[target]['test_size']} samples (Positive: {splits[target]['test_positive_ratio']:.1%})\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Execute Step 6\n",
    "print(\"\\nExecuting Step 6...\")\n",
    "\n",
    "data_splits = split_data_for_ml(ml_data_engineered, binary_flags)\n",
    "\n",
    "print(\"\\nStep 6 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176836ca-9b26-4d7f-9d0a-22d5bfc67ed4",
   "metadata": {},
   "source": [
    "### STEP 7: FINAL VALIDATION & EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a75d343-da5c-480d-8be7-4aa2f22d384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: FINAL VALIDATION & EXPORT\n",
      "\n",
      "Executing Step 7...\n",
      "\n",
      "Performing final validation checks...\n",
      "  Missing values: 0\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Exporting preprocessed datasets...\n",
      "  ml_preprocessed_full.csv\n",
      "  ml_train_High_Operative_Flag.csv & ml_test_High_Operative_Flag.csv\n",
      "  ml_train_High_Growth_Flag.csv & ml_test_High_Growth_Flag.csv\n",
      "  ml_train_High_RuPay_Flag.csv & ml_test_High_RuPay_Flag.csv\n",
      "  ml_train_High_Balance_Flag.csv & ml_test_High_Balance_Flag.csv\n",
      "  ml_train_Rural_Dominated_Flag.csv & ml_test_Rural_Dominated_Flag.csv\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING PIPELINE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final Dataset Summary:\n",
      "  Total samples: 36\n",
      "  Total features: 61\n",
      "  Engineered features: 8\n",
      "  Binary targets: 5\n",
      "  Balanced datasets created: 0\n",
      "\n",
      "ALL PREPROCESSING STEPS COMPLETED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 7: FINAL VALIDATION & EXPORT\")\n",
    "\n",
    "def validate_and_export(df_processed, splits, balanced_data):\n",
    "    \"\"\"Perform final validation and export preprocessed data\"\"\"\n",
    "    \n",
    "    print(\"\\nPerforming final validation checks...\")\n",
    "    \n",
    "    # Check 1: No missing values\n",
    "    missing_count = df_processed.isnull().sum().sum()\n",
    "    print(f\"  Missing values: {missing_count}\")\n",
    "    \n",
    "    # Check 2: All numerical features scaled\n",
    "    numerical_cols = df_processed.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numerical_cols:\n",
    "        if 'Flag' not in col and 'Region_' not in col:\n",
    "            col_std = df_processed[col].std()\n",
    "            col_mean = df_processed[col].mean()\n",
    "            if abs(col_mean) > 10 or col_std > 10:\n",
    "                print(f\"  ⚠ Warning: {col} may not be properly scaled (mean={col_mean:.2f}, std={col_std:.2f})\")\n",
    "    \n",
    "    # Check 3: Binary flags intact\n",
    "    binary_cols = [col for col in df_processed.columns if 'Flag' in col]\n",
    "    for col in binary_cols:\n",
    "        unique_vals = df_processed[col].unique()\n",
    "        if not set(unique_vals).issubset({0, 1}):\n",
    "            print(f\"  ⚠ Warning: {col} has non-binary values: {unique_vals}\")\n",
    "    \n",
    "    print(\"\\nValidation complete\")\n",
    "    \n",
    "    # Export preprocessed data\n",
    "    print(\"\\nExporting preprocessed datasets...\")\n",
    "    \n",
    "    # 1. Main preprocessed dataset\n",
    "    df_processed.to_csv('ml_preprocessed_full.csv', index=False)\n",
    "    print(\"  ml_preprocessed_full.csv\")\n",
    "    \n",
    "    # 2. Export train-test splits for each target\n",
    "    for target, split_data in splits.items():\n",
    "        # Train set\n",
    "        train_df = pd.DataFrame(split_data['X_train'])\n",
    "        train_df[target] = split_data['y_train']\n",
    "        train_df.to_csv(f'ml_train_{target}.csv', index=False)\n",
    "        \n",
    "        # Test set\n",
    "        test_df = pd.DataFrame(split_data['X_test'])\n",
    "        test_df[target] = split_data['y_test']\n",
    "        test_df.to_csv(f'ml_test_{target}.csv', index=False)\n",
    "        \n",
    "        print(f\"  ml_train_{target}.csv & ml_test_{target}.csv\")\n",
    "    \n",
    "    # 3. Export balanced datasets for imbalanced targets\n",
    "    for target, balanced in balanced_data.items():\n",
    "        balanced_df = pd.DataFrame(balanced['X'])\n",
    "        balanced_df[target] = balanced['y']\n",
    "        balanced_df.to_csv(f'ml_balanced_{target}.csv', index=False)\n",
    "        print(f\"  ml_balanced_{target}.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPROCESSING PIPELINE COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nFinal Dataset Summary:\")\n",
    "    print(f\"  Total samples: {len(df_processed)}\")\n",
    "    print(f\"  Total features: {len(df_processed.columns)}\")\n",
    "    print(f\"  Engineered features: {len([c for c in df_processed.columns if 'Interaction' in c or 'Squared' in c or 'Region_' in c])}\")\n",
    "    print(f\"  Binary targets: {len(binary_cols)}\")\n",
    "    print(f\"  Balanced datasets created: {len(balanced_data)}\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Execute Step 7\n",
    "print(\"\\nExecuting Step 7...\")\n",
    "\n",
    "final_data = validate_and_export(ml_data_engineered, data_splits, balanced_data)\n",
    "\n",
    "print(\"\\nALL PREPROCESSING STEPS COMPLETED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86885b-1425-42de-854a-94f654d2060a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
