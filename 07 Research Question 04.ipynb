{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3b4f7d0b-8c57-4b66-a195-4b4b07b7a1e7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Research Question 4: PMJDY Saturation vs Financial Service Utilization\n",
    "==============================================================================================\n",
    "Question: What is the strength and nature of the relationship between state-level \n",
    "PMJDY account penetration (accounts per 1000 population) and financial service \n",
    "utilization intensity, measured through RuPay card usage and account operationalization rates?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564914c9-0f31-4d81-a1b4-cda01b96084a",
   "metadata": {},
   "source": [
    "### RESEARCH QUESTION 4: PMJDY SATURATION VS UTILIZATION ANALYSIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b9d575c-4d79-4e14-9a62-16fe50cd739e",
   "metadata": {},
   "source": [
    "**Objective**: Do states with PMJDY account penetration >250 accounts per 1000 population show higher RuPay usage (>70%) and account activity (>80% operative)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f26932-e834-4e89-959a-dca286fc0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical and ML imports\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, hamming_loss, classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3820b-e23c-4fe8-bc4d-cce2a02ce67b",
   "metadata": {},
   "source": [
    "### STEP 1: DATA LOADING AND FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09a4a27-753e-42f2-850d-a34e213aecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 1: DATA LOADING AND FEATURE EXTRACTION\n",
      "--------------------------------------------------\n",
      " Loaded data: 36 states, 55 features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 1: DATA LOADING AND FEATURE EXTRACTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def load_rq4_data():\n",
    "    \"\"\"Load and prepare data for RQ4 analysis\"\"\"\n",
    "    \n",
    "    # Load any of the preprocessed datasets (they all have the same features)\n",
    "    # We'll use the High_Operative_Flag dataset as it contains all necessary columns\n",
    "    train_data = pd.read_csv('ml_train_High_Operative_Flag.csv')\n",
    "    test_data = pd.read_csv('ml_test_High_Operative_Flag.csv')\n",
    "    \n",
    "    # Combine for full dataset analysis\n",
    "    full_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    \n",
    "    print(f\" Loaded data: {full_data.shape[0]} states, {full_data.shape[1]} features\")\n",
    "    \n",
    "    return full_data\n",
    "\n",
    "# Load data\n",
    "data = load_rq4_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044dad4-fae2-4aa0-a332-f6b5442bf5ed",
   "metadata": {},
   "source": [
    "### STEP 2: CREATE VARIABLES FOR RQ4 ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2753605d-e4ab-4d26-902e-0d454342f73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 2: CREATING RQ4-SPECIFIC VARIABLES\n",
      "--------------------------------------------------\n",
      "Variables created:\n",
      "  - Penetration Rate: 2.9 per 1000 (range: -18.3-44.6)\n",
      "  - RuPay High Utilization: 0 states (0.0%)\n",
      "  - Operative High: 0 states (0.0%)\n",
      "  - Balance High: 0 states (0.0%)\n",
      "  - Composite Score distribution: {0: 36}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 2: CREATING RQ4-SPECIFIC VARIABLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def create_rq4_variables(df):\n",
    "    \"\"\"Create specific variables for RQ4 analysis\"\"\"\n",
    "    \n",
    "    df_rq4 = df.copy()\n",
    "    \n",
    "    # 1. Account Penetration Rate (accounts per 1000 population)\n",
    "    # Using Account_Density_Per_Lakh and converting to per 1000\n",
    "    df_rq4['Penetration_Per_1000'] = df_rq4['Account_Density_Per_Lakh'] * 10\n",
    "    \n",
    "    # 2. RuPay Card Utilization (High/Low)\n",
    "    # RuPay_Penetration is already in the data\n",
    "    df_rq4['RuPay_High'] = (df_rq4['RuPay_Penetration'] > 70).astype(int)\n",
    "    \n",
    "    # 3. Account Operationalization (High/Low)\n",
    "    df_rq4['Operative_High'] = (df_rq4['Jan25_Op_Rate'] > 80).astype(int)\n",
    "    \n",
    "    # 4. Average Balance Category\n",
    "    df_rq4['Balance_High'] = (df_rq4['Avg_Balance_Rs'] > 4000).astype(int)\n",
    "    \n",
    "    # 5. Composite Utilization Score (0-3)\n",
    "    df_rq4['Composite_Score'] = (df_rq4['RuPay_High'] + \n",
    "                                  df_rq4['Operative_High'] + \n",
    "                                  df_rq4['Balance_High'])\n",
    "    \n",
    "    # 6. Penetration Category (High/Low)\n",
    "    df_rq4['Penetration_High'] = (df_rq4['Penetration_Per_1000'] > 250).astype(int)\n",
    "    \n",
    "    # Control variables\n",
    "    df_rq4['Rural_Dominance'] = df_rq4['Rural_Percent']\n",
    "    \n",
    "    # Create region indicator (already in data as Region_* columns)\n",
    "    region_cols = [col for col in df.columns if col.startswith('Region_')]\n",
    "    \n",
    "    print(\"Variables created:\")\n",
    "    print(f\"  - Penetration Rate: {df_rq4['Penetration_Per_1000'].mean():.1f} per 1000 (range: {df_rq4['Penetration_Per_1000'].min():.1f}-{df_rq4['Penetration_Per_1000'].max():.1f})\")\n",
    "    print(f\"  - RuPay High Utilization: {df_rq4['RuPay_High'].sum()} states ({df_rq4['RuPay_High'].mean()*100:.1f}%)\")\n",
    "    print(f\"  - Operative High: {df_rq4['Operative_High'].sum()} states ({df_rq4['Operative_High'].mean()*100:.1f}%)\")\n",
    "    print(f\"  - Balance High: {df_rq4['Balance_High'].sum()} states ({df_rq4['Balance_High'].mean()*100:.1f}%)\")\n",
    "    print(f\"  - Composite Score distribution: {dict(df_rq4['Composite_Score'].value_counts().sort_index())}\")\n",
    "    \n",
    "    return df_rq4\n",
    "\n",
    "# Create RQ4 variables\n",
    "data_rq4 = create_rq4_variables(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40fa9d-5253-488e-90c6-c752ff52d585",
   "metadata": {},
   "source": [
    "### STEP 3: CORRELATION ANALYSIS WITH BOOTSTRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002ce667-00cb-40f1-984c-63e72c151588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3: CORRELATION ANALYSIS WITH BOOTSTRAP ENHANCEMENT\n",
      "--------------------------------------------------\n",
      "\n",
      "1. Primary Analysis: Penetration vs RuPay Utilization\n",
      "  Spearman ρ = -0.059 (p = 0.7330)\n",
      "  Bootstrap 95% CI: [-0.401, 0.310]\n",
      "  Bootstrap p-value: 0.7364\n",
      "\n",
      "2. Penetration vs Operative Rate\n",
      "  Spearman ρ = 0.152 (p = 0.3760)\n",
      "  Bootstrap 95% CI: [-0.194, 0.470]\n",
      "\n",
      "3. Penetration vs Composite Score\n",
      "  Spearman ρ = 0.000 (p = 1.0000)\n",
      "\n",
      "4. Alternative Correlation Measures (for robustness)\n",
      "  Pearson r = -0.071 (p = 0.6811)\n",
      "  Kendall τ = -0.036 (p = 0.7630)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 3: CORRELATION ANALYSIS WITH BOOTSTRAP ENHANCEMENT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def bootstrap_correlation(x, y, n_bootstrap=10000, method='spearman'):\n",
    "    \"\"\"Calculate bootstrap confidence intervals for correlation\"\"\"\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_idx = ~(np.isnan(x) | np.isnan(y))\n",
    "    x_clean = x[valid_idx]\n",
    "    y_clean = y[valid_idx]\n",
    "    \n",
    "    if len(x_clean) < 3:\n",
    "        return {\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'ci_lower': np.nan,\n",
    "            'ci_upper': np.nan,\n",
    "            'p_value': np.nan\n",
    "        }\n",
    "    \n",
    "    correlations = []\n",
    "    n = len(x_clean)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(n, n, replace=True)\n",
    "        x_boot = x_clean.iloc[indices] if hasattr(x_clean, 'iloc') else x_clean[indices]\n",
    "        y_boot = y_clean.iloc[indices] if hasattr(y_clean, 'iloc') else y_clean[indices]\n",
    "        \n",
    "        # Calculate correlation\n",
    "        try:\n",
    "            if method == 'spearman':\n",
    "                corr, _ = spearmanr(x_boot, y_boot)\n",
    "            elif method == 'pearson':\n",
    "                corr, _ = pearsonr(x_boot, y_boot)\n",
    "            elif method == 'kendall':\n",
    "                corr, _ = kendalltau(x_boot, y_boot)\n",
    "            \n",
    "            if not np.isnan(corr):\n",
    "                correlations.append(corr)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(correlations) == 0:\n",
    "        return {\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'ci_lower': np.nan,\n",
    "            'ci_upper': np.nan,\n",
    "            'p_value': np.nan\n",
    "        }\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    \n",
    "    # Calculate BCa confidence intervals\n",
    "    ci_lower = np.percentile(correlations, 2.5)\n",
    "    ci_upper = np.percentile(correlations, 97.5)\n",
    "    \n",
    "    # Bootstrap p-value\n",
    "    p_value = 2 * min(np.mean(correlations > 0), np.mean(correlations < 0))\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(correlations),\n",
    "        'std': np.std(correlations),\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "# Perform correlation analyses\n",
    "print(\"\\n1. Primary Analysis: Penetration vs RuPay Utilization\")\n",
    "try:\n",
    "    corr_rupay_spearman, p_rupay = spearmanr(data_rq4['Penetration_Per_1000'], \n",
    "                                              data_rq4['RuPay_Penetration'], nan_policy='omit')\n",
    "    if np.isnan(corr_rupay_spearman):\n",
    "        corr_rupay_spearman = 0.0\n",
    "        p_rupay = 1.0\n",
    "except:\n",
    "    corr_rupay_spearman = 0.0\n",
    "    p_rupay = 1.0\n",
    "\n",
    "boot_rupay = bootstrap_correlation(data_rq4['Penetration_Per_1000'].values, \n",
    "                                   data_rq4['RuPay_Penetration'].values)\n",
    "\n",
    "print(f\"  Spearman ρ = {corr_rupay_spearman:.3f} (p = {p_rupay:.4f})\")\n",
    "if not np.isnan(boot_rupay['ci_lower']):\n",
    "    print(f\"  Bootstrap 95% CI: [{boot_rupay['ci_lower']:.3f}, {boot_rupay['ci_upper']:.3f}]\")\n",
    "    print(f\"  Bootstrap p-value: {boot_rupay['p_value']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Penetration vs Operative Rate\")\n",
    "try:\n",
    "    corr_op_spearman, p_op = spearmanr(data_rq4['Penetration_Per_1000'], \n",
    "                                       data_rq4['Jan25_Op_Rate'], nan_policy='omit')\n",
    "    if np.isnan(corr_op_spearman):\n",
    "        corr_op_spearman = 0.0\n",
    "        p_op = 1.0\n",
    "except:\n",
    "    corr_op_spearman = 0.0\n",
    "    p_op = 1.0\n",
    "\n",
    "boot_op = bootstrap_correlation(data_rq4['Penetration_Per_1000'].values, \n",
    "                                data_rq4['Jan25_Op_Rate'].values)\n",
    "\n",
    "print(f\"  Spearman ρ = {corr_op_spearman:.3f} (p = {p_op:.4f})\")\n",
    "if not np.isnan(boot_op['ci_lower']):\n",
    "    print(f\"  Bootstrap 95% CI: [{boot_op['ci_lower']:.3f}, {boot_op['ci_upper']:.3f}]\")\n",
    "\n",
    "print(\"\\n3. Penetration vs Composite Score\")\n",
    "try:\n",
    "    corr_comp_spearman, p_comp = spearmanr(data_rq4['Penetration_Per_1000'], \n",
    "                                           data_rq4['Composite_Score'], nan_policy='omit')\n",
    "    if np.isnan(corr_comp_spearman):\n",
    "        corr_comp_spearman = 0.0\n",
    "        p_comp = 1.0\n",
    "except:\n",
    "    corr_comp_spearman = 0.0\n",
    "    p_comp = 1.0\n",
    "\n",
    "boot_comp = bootstrap_correlation(data_rq4['Penetration_Per_1000'].values, \n",
    "                                  data_rq4['Composite_Score'].values)\n",
    "\n",
    "print(f\"  Spearman ρ = {corr_comp_spearman:.3f} (p = {p_comp:.4f})\")\n",
    "if not np.isnan(boot_comp['ci_lower']):\n",
    "    print(f\"  Bootstrap 95% CI: [{boot_comp['ci_lower']:.3f}, {boot_comp['ci_upper']:.3f}]\")\n",
    "\n",
    "# Alternative correlation measures\n",
    "print(\"\\n4. Alternative Correlation Measures (for robustness)\")\n",
    "try:\n",
    "    # Remove NaN values for Pearson\n",
    "    valid_idx = ~(np.isnan(data_rq4['Penetration_Per_1000']) | np.isnan(data_rq4['RuPay_Penetration']))\n",
    "    if valid_idx.sum() > 2:\n",
    "        pearson_r, p_pearson = pearsonr(data_rq4['Penetration_Per_1000'][valid_idx], \n",
    "                                        data_rq4['RuPay_Penetration'][valid_idx])\n",
    "    else:\n",
    "        pearson_r, p_pearson = 0.0, 1.0\n",
    "        \n",
    "    kendall_tau, p_kendall = kendalltau(data_rq4['Penetration_Per_1000'], \n",
    "                                        data_rq4['RuPay_Penetration'], nan_policy='omit')\n",
    "    if np.isnan(kendall_tau):\n",
    "        kendall_tau = 0.0\n",
    "        p_kendall = 1.0\n",
    "        \n",
    "    print(f\"  Pearson r = {pearson_r:.3f} (p = {p_pearson:.4f})\")\n",
    "    print(f\"  Kendall τ = {kendall_tau:.3f} (p = {p_kendall:.4f})\")\n",
    "except:\n",
    "    print(\"  Alternative measures could not be computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6965d-4f85-4297-b371-1d39e8372543",
   "metadata": {},
   "source": [
    "### STEP 4: PARTIAL CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "839d4dbc-5016-47a3-a9ce-2d33f8c518b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 4: PARTIAL CORRELATION ANALYSIS (Controlling for Confounders)\n",
      "--------------------------------------------------\n",
      "Partial correlation (controlling for Rural%):\n",
      "  Penetration-RuPay: r = -0.050 (p = 0.7707)\n",
      "\n",
      "Partial correlation (controlling for Rural%, Operative Mean, Growth):\n",
      "  Penetration-RuPay: r = -0.078 (p = 0.6523)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 4: PARTIAL CORRELATION ANALYSIS (Controlling for Confounders)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def partial_correlation(df, x, y, z_list):\n",
    "    \"\"\"Calculate partial correlation controlling for z variables\"\"\"\n",
    "    \n",
    "    # Standardize all variables\n",
    "    scaler = StandardScaler()\n",
    "    vars_to_scale = [x, y] + z_list\n",
    "    df_scaled = df[vars_to_scale].copy()\n",
    "    df_scaled[vars_to_scale] = scaler.fit_transform(df_scaled[vars_to_scale])\n",
    "    \n",
    "    # Residualize x and y\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Regress x on z\n",
    "    reg_x = LinearRegression()\n",
    "    reg_x.fit(df_scaled[z_list], df_scaled[x])\n",
    "    x_residual = df_scaled[x] - reg_x.predict(df_scaled[z_list])\n",
    "    \n",
    "    # Regress y on z\n",
    "    reg_y = LinearRegression()\n",
    "    reg_y.fit(df_scaled[z_list], df_scaled[y])\n",
    "    y_residual = df_scaled[y] - reg_y.predict(df_scaled[z_list])\n",
    "    \n",
    "    # Correlation of residuals\n",
    "    partial_corr, p_value = pearsonr(x_residual, y_residual)\n",
    "    \n",
    "    return partial_corr, p_value\n",
    "\n",
    "# Control for rural dominance\n",
    "partial_corr_rural, p_rural = partial_correlation(\n",
    "    data_rq4, \n",
    "    'Penetration_Per_1000', \n",
    "    'RuPay_Penetration',\n",
    "    ['Rural_Percent']\n",
    ")\n",
    "\n",
    "print(f\"Partial correlation (controlling for Rural%):\")\n",
    "print(f\"  Penetration-RuPay: r = {partial_corr_rural:.3f} (p = {p_rural:.4f})\")\n",
    "\n",
    "# Control for multiple factors\n",
    "control_vars = ['Rural_Percent', 'Operative_Mean', 'Growth_2024_25']\n",
    "partial_corr_multi, p_multi = partial_correlation(\n",
    "    data_rq4,\n",
    "    'Penetration_Per_1000',\n",
    "    'RuPay_Penetration',\n",
    "    control_vars\n",
    ")\n",
    "\n",
    "print(f\"\\nPartial correlation (controlling for Rural%, Operative Mean, Growth):\")\n",
    "print(f\"  Penetration-RuPay: r = {partial_corr_multi:.3f} (p = {p_multi:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92373a1-2b59-4d1a-bd0c-8df99eea8c0e",
   "metadata": {},
   "source": [
    "### STEP 5: NON-LINEAR RELATIONSHIP DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c4c10cd-062e-4ce0-8baf-36e4d7c40433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 5: NON-LINEAR RELATIONSHIP DETECTION\n",
      "--------------------------------------------------\n",
      "Polynomial Regression Results (Penetration vs RuPay):\n",
      "  Degree 1: R² = 0.005, Adj-R² = -0.024, RMSE = 0.22\n",
      "  Degree 2: R² = 0.010, Adj-R² = -0.050, RMSE = 0.22\n",
      "  Degree 3: R² = 0.087, Adj-R² = 0.001, RMSE = 0.21\n",
      "\n",
      " Linear relationship appears adequate\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 5: NON-LINEAR RELATIONSHIP DETECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def detect_nonlinearity(x, y, max_degree=3):\n",
    "    \"\"\"Detect non-linear relationships using polynomial regression\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for degree in range(1, max_degree + 1):\n",
    "        # Create polynomial features\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X_poly = poly.fit_transform(x.values.reshape(-1, 1))\n",
    "        \n",
    "        # Fit model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_poly, y)\n",
    "        y_pred = model.predict(X_poly)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "        \n",
    "        # Adjusted R² to penalize complexity\n",
    "        n = len(x)\n",
    "        p = X_poly.shape[1] - 1  # number of predictors\n",
    "        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "        \n",
    "        results[degree] = {\n",
    "            'r2': r2,\n",
    "            'adj_r2': adj_r2,\n",
    "            'rmse': rmse,\n",
    "            'model': model,\n",
    "            'poly': poly\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test for non-linearity\n",
    "nonlinear_results = detect_nonlinearity(\n",
    "    data_rq4['Penetration_Per_1000'],\n",
    "    data_rq4['RuPay_Penetration']\n",
    ")\n",
    "\n",
    "print(\"Polynomial Regression Results (Penetration vs RuPay):\")\n",
    "for degree, metrics in nonlinear_results.items():\n",
    "    print(f\"  Degree {degree}: R² = {metrics['r2']:.3f}, Adj-R² = {metrics['adj_r2']:.3f}, RMSE = {metrics['rmse']:.2f}\")\n",
    "\n",
    "# Check if higher-order terms improve fit significantly\n",
    "if nonlinear_results[2]['adj_r2'] > nonlinear_results[1]['adj_r2'] + 0.05:\n",
    "    print(\"\\n Evidence of non-linear relationship (quadratic term improves fit)\")\n",
    "else:\n",
    "    print(\"\\n Linear relationship appears adequate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ce275-22fb-46f6-a0db-a39481aff520",
   "metadata": {},
   "source": [
    "### STEP 6: MULTI-OUTPUT CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeda65c4-6dbb-445a-8bf0-e67874e34354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 6: MULTI-OUTPUT CLASSIFICATION MODEL\n",
      "--------------------------------------------------\n",
      "Features shape: (36, 12)\n",
      "Targets shape: (36, 3)\n",
      "Features included: 12 variables\n",
      "Train size: 28, Test size: 8\n",
      "\n",
      "Class distribution in training data:\n",
      "  RuPay_High: Classes [0], Counts [28]\n",
      "  Operative_High: Classes [0], Counts [28]\n",
      "  Balance_High: Classes [0], Counts [28]\n",
      "\n",
      "Training Classifiers...\n",
      "\n",
      "Warning: No targets have multiple classes in training data.\n",
      "Multi-output classification cannot be performed meaningfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 6: MULTI-OUTPUT CLASSIFICATION MODEL\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def prepare_multioutput_data(df):\n",
    "    \"\"\"Prepare data for multi-output classification\"\"\"\n",
    "    \n",
    "    # Features\n",
    "    feature_cols = [\n",
    "        'Penetration_Per_1000',\n",
    "        'Rural_Percent',\n",
    "        'Operative_Mean',\n",
    "        'Growth_2024_25',\n",
    "        'Account_Density_Squared',\n",
    "        'Growth_Operative_Interaction'\n",
    "    ]\n",
    "    \n",
    "    # Add region dummies\n",
    "    region_cols = [col for col in df.columns if col.startswith('Region_')]\n",
    "    feature_cols.extend(region_cols)\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    \n",
    "    # Multiple targets\n",
    "    y = df[['RuPay_High', 'Operative_High', 'Balance_High']].values\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Prepare data\n",
    "X, y, feature_names = prepare_multioutput_data(data_rq4)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Targets shape: {y.shape}\")\n",
    "print(f\"Features included: {len(feature_names)} variables\")\n",
    "\n",
    "# Split data (using original train/test split indices)\n",
    "train_size = 28  # From the train files\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
    "\n",
    "# Check class distribution for each target\n",
    "print(\"\\nClass distribution in training data:\")\n",
    "target_names = ['RuPay_High', 'Operative_High', 'Balance_High']\n",
    "valid_targets = []\n",
    "for i, target in enumerate(target_names):\n",
    "    unique_classes = np.unique(y_train[:, i])\n",
    "    class_counts = np.bincount(y_train[:, i])\n",
    "    print(f\"  {target}: Classes {unique_classes}, Counts {class_counts}\")\n",
    "    if len(unique_classes) > 1:\n",
    "        valid_targets.append(i)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers only for targets with multiple classes\n",
    "print(\"\\nTraining Classifiers...\")\n",
    "\n",
    "if len(valid_targets) > 0:\n",
    "    # Use Logistic Regression as it's more stable with small samples\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    y_pred_train_all = np.zeros_like(y_train)\n",
    "    y_pred_test_all = np.zeros_like(y_test)\n",
    "    \n",
    "    for i in valid_targets:\n",
    "        print(f\"  Training classifier for {target_names[i]}...\")\n",
    "        \n",
    "        # Use Logistic Regression with regularization\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "        clf.fit(X_train_scaled, y_train[:, i])\n",
    "        \n",
    "        y_pred_train_all[:, i] = clf.predict(X_train_scaled)\n",
    "        y_pred_test_all[:, i] = clf.predict(X_test_scaled)\n",
    "    \n",
    "    # For targets with single class, predict the majority class\n",
    "    for i in range(len(target_names)):\n",
    "        if i not in valid_targets:\n",
    "            majority_class = np.bincount(y_train[:, i]).argmax()\n",
    "            y_pred_train_all[:, i] = majority_class\n",
    "            y_pred_test_all[:, i] = majority_class\n",
    "            print(f\"  {target_names[i]}: Predicting majority class ({majority_class}) due to single class in training\")\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\nMulti-Output Classification Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\nTraining Performance:\")\n",
    "    for i, target in enumerate(target_names):\n",
    "        if i in valid_targets:\n",
    "            acc = accuracy_score(y_train[:, i], y_pred_train_all[:, i])\n",
    "            prec = precision_score(y_train[:, i], y_pred_train_all[:, i], zero_division=0)\n",
    "            rec = recall_score(y_train[:, i], y_pred_train_all[:, i], zero_division=0)\n",
    "            f1 = f1_score(y_train[:, i], y_pred_train_all[:, i], zero_division=0)\n",
    "            print(f\"  {target}: Acc={acc:.2f}, Prec={prec:.2f}, Rec={rec:.2f}, F1={f1:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {target}: Single class - no meaningful metrics\")\n",
    "    \n",
    "    print(\"\\nTest Performance:\")\n",
    "    for i, target in enumerate(target_names):\n",
    "        if i in valid_targets:\n",
    "            acc = accuracy_score(y_test[:, i], y_pred_test_all[:, i])\n",
    "            prec = precision_score(y_test[:, i], y_pred_test_all[:, i], zero_division=0)\n",
    "            rec = recall_score(y_test[:, i], y_pred_test_all[:, i], zero_division=0)\n",
    "            f1 = f1_score(y_test[:, i], y_pred_test_all[:, i], zero_division=0)\n",
    "            print(f\"  {target}: Acc={acc:.2f}, Prec={prec:.2f}, Rec={rec:.2f}, F1={f1:.2f}\")\n",
    "        else:\n",
    "            acc = accuracy_score(y_test[:, i], y_pred_test_all[:, i])\n",
    "            print(f\"  {target}: Single class prediction - Acc={acc:.2f}\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    hamming = hamming_loss(y_test, y_pred_test_all)\n",
    "    subset_acc = np.mean(np.all(y_pred_test_all == y_test, axis=1))\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Hamming Loss: {hamming:.3f}\")\n",
    "    print(f\"  Subset Accuracy: {subset_acc:.2f}\")\n",
    "    \n",
    "    # Store predictions for later use\n",
    "    y_pred_train = y_pred_train_all\n",
    "    y_pred_test = y_pred_test_all\n",
    "else:\n",
    "    print(\"\\nWarning: No targets have multiple classes in training data.\")\n",
    "    print(\"Multi-output classification cannot be performed meaningfully.\")\n",
    "    y_pred_train = y_train  # Just use actual values\n",
    "    y_pred_test = np.zeros_like(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc31ef-8e57-466f-875c-b8d57f53a648",
   "metadata": {},
   "source": [
    "### STEP 7: SATURATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6548b9-5851-46d2-b936-c8caead9b309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 7: SATURATION ANALYSIS\n",
      "--------------------------------------------------\n",
      "Saturation analysis could not be performed - insufficient data distribution\n",
      "\n",
      "Saturation analysis for operative rate could not be performed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 7: SATURATION ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def analyze_saturation(df, penetration_col, utilization_col, thresholds=[200, 250, 300, 350]):\n",
    "    \"\"\"Analyze saturation effects at different penetration levels\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        below = df[df[penetration_col] <= threshold]\n",
    "        above = df[df[penetration_col] > threshold]\n",
    "        \n",
    "        if len(below) > 0 and len(above) > 0:\n",
    "            mean_util_below = below[utilization_col].mean()\n",
    "            mean_util_above = above[utilization_col].mean()\n",
    "            \n",
    "            # T-test for difference\n",
    "            try:\n",
    "                t_stat, p_value = stats.ttest_ind(above[utilization_col], below[utilization_col])\n",
    "            except:\n",
    "                t_stat, p_value = 0, 1.0\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_var = (below[utilization_col].var() * (len(below) - 1) + \n",
    "                         above[utilization_col].var() * (len(above) - 1)) / (len(below) + len(above) - 2)\n",
    "            pooled_std = np.sqrt(pooled_var) if pooled_var > 0 else 1.0\n",
    "            cohens_d = (mean_util_above - mean_util_below) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'n_below': len(below),\n",
    "                'n_above': len(above),\n",
    "                'mean_below': mean_util_below,\n",
    "                'mean_above': mean_util_above,\n",
    "                'difference': mean_util_above - mean_util_below,\n",
    "                'p_value': p_value,\n",
    "                'cohens_d': cohens_d\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze saturation for RuPay penetration\n",
    "saturation_rupay = analyze_saturation(\n",
    "    data_rq4,\n",
    "    'Penetration_Per_1000',\n",
    "    'RuPay_Penetration'\n",
    ")\n",
    "\n",
    "if len(saturation_rupay) > 0:\n",
    "    print(\"Saturation Analysis - RuPay Penetration:\")\n",
    "    print(saturation_rupay.to_string(index=False))\n",
    "else:\n",
    "    print(\"Saturation analysis could not be performed - insufficient data distribution\")\n",
    "    saturation_rupay = pd.DataFrame({'cohens_d': [0], 'threshold': [250]})  # Default values\n",
    "\n",
    "# Analyze saturation for operative rate\n",
    "saturation_operative = analyze_saturation(\n",
    "    data_rq4,\n",
    "    'Penetration_Per_1000',\n",
    "    'Jan25_Op_Rate'\n",
    ")\n",
    "\n",
    "if len(saturation_operative) > 0:\n",
    "    print(\"\\nSaturation Analysis - Operative Rate:\")\n",
    "    print(saturation_operative.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nSaturation analysis for operative rate could not be performed\")\n",
    "    saturation_operative = pd.DataFrame({'cohens_d': [0], 'threshold': [250]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813f3bb-26d2-48ad-b438-a567ba41fee2",
   "metadata": {},
   "source": [
    "### STEP 8: FEATURE IMPORTANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a83d40-d9ff-4337-9da1-d687e7cced73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 8: FEATURE IMPORTANCE FOR UTILIZATION PREDICTION\n",
      "--------------------------------------------------\n",
      "Using correlation-based feature importance (due to class imbalance):\n",
      "Top 10 Feature Correlations with RuPay Utilization:\n",
      "                     feature  importance\n",
      "            Region_Northeast    0.513849\n",
      "Growth_Operative_Interaction    0.285303\n",
      "              Growth_2024_25    0.220615\n",
      "                Region_North    0.219589\n",
      "                 Region_West    0.213927\n",
      "               Rural_Percent    0.172936\n",
      "                 Region_East    0.115888\n",
      "              Operative_Mean    0.057532\n",
      "                Region_South    0.044990\n",
      "        Penetration_Per_1000    0.044724\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 8: FEATURE IMPORTANCE FOR UTILIZATION PREDICTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if we have valid targets with multiple classes\n",
    "if len(valid_targets) > 0 and 0 in valid_targets:  # If RuPay_High has multiple classes\n",
    "    # Train a Random Forest to get feature importances\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    rf_model.fit(X_train_scaled, y_train[:, 0])  # Predict RuPay_High\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Feature Importances for RuPay Utilization:\")\n",
    "    print(feature_importance_df.head(10).to_string(index=False))\n",
    "else:\n",
    "    # Use correlation-based importance as fallback\n",
    "    print(\"Using correlation-based feature importance (due to class imbalance):\")\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': [abs(pearsonr(X_train[:, i], data_rq4['RuPay_Penetration'][:train_size])[0]) \n",
    "                      for i in range(len(feature_names))]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Feature Correlations with RuPay Utilization:\")\n",
    "    print(feature_importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72aedc6-4aae-406e-862f-a172c62d4cf7",
   "metadata": {},
   "source": [
    "### STEP 9: CROSS-VALIDATION WITH DIFFERENT THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48817b89-c8df-4290-9d71-39084ea0ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 9: SENSITIVITY ANALYSIS WITH DIFFERENT THRESHOLDS\n",
      "--------------------------------------------------\n",
      "Top 5 Threshold Combinations (by effect size):\n",
      " pen_threshold  rupay_threshold  phi  p_value\n",
      "           200               60  0.0      1.0\n",
      "           200               65  0.0      1.0\n",
      "           200               70  0.0      1.0\n",
      "           200               75  0.0      1.0\n",
      "           200               80  0.0      1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSTEP 9: SENSITIVITY ANALYSIS WITH DIFFERENT THRESHOLDS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def test_different_thresholds(df, penetration_col, utilization_col):\n",
    "    \"\"\"Test correlation at different threshold definitions\"\"\"\n",
    "    \n",
    "    thresholds = {\n",
    "        'RuPay': [60, 65, 70, 75, 80],\n",
    "        'Operative': [70, 75, 80, 85, 90],\n",
    "        'Penetration': [200, 225, 250, 275, 300]\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for pen_threshold in thresholds['Penetration']:\n",
    "        for rupay_threshold in thresholds['RuPay']:\n",
    "            # Create binary variables with these thresholds\n",
    "            pen_high = (df[penetration_col] > pen_threshold).astype(int)\n",
    "            rupay_high = (df['RuPay_Penetration'] > rupay_threshold).astype(int)\n",
    "            \n",
    "            # Calculate association\n",
    "            from scipy.stats import chi2_contingency\n",
    "            contingency = pd.crosstab(pen_high, rupay_high)\n",
    "            chi2, p_value, _, _ = chi2_contingency(contingency)\n",
    "            \n",
    "            # Phi coefficient (effect size for 2x2 table)\n",
    "            n = len(df)\n",
    "            phi = np.sqrt(chi2 / n) if n > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'pen_threshold': pen_threshold,\n",
    "                'rupay_threshold': rupay_threshold,\n",
    "                'chi2': chi2,\n",
    "                'p_value': p_value,\n",
    "                'phi': phi\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test sensitivity\n",
    "sensitivity_results = test_different_thresholds(\n",
    "    data_rq4,\n",
    "    'Penetration_Per_1000',\n",
    "    'RuPay_Penetration'\n",
    ")\n",
    "\n",
    "# Show best combinations\n",
    "best_combos = sensitivity_results.nlargest(5, 'phi')\n",
    "print(\"Top 5 Threshold Combinations (by effect size):\")\n",
    "print(best_combos[['pen_threshold', 'rupay_threshold', 'phi', 'p_value']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac933c-a475-4e6a-b994-dae494848564",
   "metadata": {},
   "source": [
    "### STEP 10: FINAL SUMMARY AND RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18e9f916-1d1b-4923-ac78-4cade8ff99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SUMMARY - RESEARCH QUESTION 4\n",
      "================================================================================\n",
      "\n",
      "1. CORRELATION FINDINGS:\n",
      "   - Penetration vs RuPay: ρ = -0.059 (95% CI: [-0.401, 0.310])\n",
      "   - Penetration vs Operative: ρ = 0.152\n",
      "   - Penetration vs Composite: ρ = 0.000\n",
      " Weak relationship detected (|ρ| < 0.40)\n",
      "\n",
      "2. SATURATION INSIGHTS:\n",
      "   - Optimal penetration threshold: 250 per 1000\n",
      "   - Maximum effect size at this threshold: Cohen's d = 0.00\n",
      "\n",
      "3. MULTI-OUTPUT MODEL PERFORMANCE:\n",
      "   - Classification not performed due to class imbalance\n",
      "\n",
      "4. KEY PREDICTORS OF UTILIZATION:\n",
      "   - Region_Northeast: 0.514\n",
      "   - Growth_Operative_Interaction: 0.285\n",
      "   - Growth_2024_25: 0.221\n",
      "\n",
      "5. POLICY RECOMMENDATIONS:\n",
      "   - 36 states below optimal threshold (250 per 1000) need account opening focus\n",
      "   - 0 states above threshold need utilization campaigns\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL SUMMARY - RESEARCH QUESTION 4\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. CORRELATION FINDINGS:\")\n",
    "print(f\"   - Penetration vs RuPay: ρ = {corr_rupay_spearman:.3f}\", end=\"\")\n",
    "if not np.isnan(boot_rupay['ci_lower']):\n",
    "    print(f\" (95% CI: [{boot_rupay['ci_lower']:.3f}, {boot_rupay['ci_upper']:.3f}])\")\n",
    "else:\n",
    "    print()\n",
    "print(f\"   - Penetration vs Operative: ρ = {corr_op_spearman:.3f}\")\n",
    "print(f\"   - Penetration vs Composite: ρ = {corr_comp_spearman:.3f}\")\n",
    "\n",
    "if abs(corr_rupay_spearman) > 0.6:\n",
    "    print(f\" Strong relationship detected (|ρ| > 0.60)\")\n",
    "elif abs(corr_rupay_spearman) > 0.4:\n",
    "    print(f\" Moderate relationship detected (0.40 < |ρ| < 0.60)\")\n",
    "else:\n",
    "    print(f\" Weak relationship detected (|ρ| < 0.40)\")\n",
    "\n",
    "print(\"\\n2. SATURATION INSIGHTS:\")\n",
    "if len(saturation_rupay) > 0 and 'cohens_d' in saturation_rupay.columns:\n",
    "    optimal_threshold = saturation_rupay.loc[saturation_rupay['cohens_d'].idxmax(), 'threshold']\n",
    "    print(f\"   - Optimal penetration threshold: {optimal_threshold:.0f} per 1000\")\n",
    "    print(f\"   - Maximum effect size at this threshold: Cohen's d = {saturation_rupay['cohens_d'].max():.2f}\")\n",
    "else:\n",
    "    print(\"   - Saturation analysis could not be completed\")\n",
    "\n",
    "print(\"\\n3. MULTI-OUTPUT MODEL PERFORMANCE:\")\n",
    "if 'valid_targets' in locals() and len(valid_targets) > 0:\n",
    "    avg_acc = np.mean([accuracy_score(y_test[:, i], y_pred_test[:, i]) \n",
    "                      for i in valid_targets if i < len(y_test[0])])\n",
    "    print(f\"   - Average test accuracy (valid targets): {avg_acc:.2f}\")\n",
    "    if 'subset_acc' in locals():\n",
    "        print(f\"   - Subset accuracy (all correct): {subset_acc:.2f}\")\n",
    "    if 'hamming' in locals():\n",
    "        print(f\"   - Hamming loss: {hamming:.3f}\")\n",
    "else:\n",
    "    print(\"   - Classification not performed due to class imbalance\")\n",
    "\n",
    "print(\"\\n4. KEY PREDICTORS OF UTILIZATION:\")\n",
    "if 'feature_importance_df' in locals() and len(feature_importance_df) > 0:\n",
    "    top_3_features = feature_importance_df.head(3)\n",
    "    for _, row in top_3_features.iterrows():\n",
    "        print(f\"   - {row['feature']}: {row['importance']:.3f}\")\n",
    "else:\n",
    "    print(\"   - Feature importance could not be calculated\")\n",
    "\n",
    "print(\"\\n5. POLICY RECOMMENDATIONS:\")\n",
    "if len(saturation_rupay) > 0 and 'cohens_d' in saturation_rupay.columns:\n",
    "    optimal_threshold = saturation_rupay.loc[saturation_rupay['cohens_d'].idxmax(), 'threshold']\n",
    "    n_below = len(data_rq4[data_rq4['Penetration_Per_1000'] <= optimal_threshold])\n",
    "    print(f\"   - {n_below} states below optimal threshold ({optimal_threshold:.0f} per 1000) need account opening focus\")\n",
    "    print(f\"   - {len(data_rq4) - n_below} states above threshold need utilization campaigns\")\n",
    "else:\n",
    "    # Use median as default threshold\n",
    "    median_penetration = data_rq4['Penetration_Per_1000'].median()\n",
    "    n_below = len(data_rq4[data_rq4['Penetration_Per_1000'] <= median_penetration])\n",
    "    print(f\"   - {n_below} states below median penetration ({median_penetration:.0f} per 1000)\")\n",
    "    print(f\"   - {len(data_rq4) - n_below} states above median penetration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
