{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15c9758-435c-4e8d-95f7-2f5f2c4536a9",
   "metadata": {},
   "source": [
    "### Research Question 3: Clustering States into PMJDY Performance Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6f6c9-0ee1-4358-a08c-5ae1db5698dc",
   "metadata": {},
   "source": [
    "**Objective**: Can we identify distinct state profiles based on financial inclusion metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cca189-9a59-4a80-8386-59dda16072ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import f_oneway, kruskal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73ebba1-d255-42bc-b1f4-76c7938b04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179da6a-7b95-4dec-9027-ff177320a7c8",
   "metadata": {},
   "source": [
    "### STEP 1: DATA LOADING AND FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463cb1c4-870f-4f70-9b2b-79ec882a27ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: DATA LOADING AND FEATURE SELECTION\n",
      "--------------------------------------------------\n",
      " Loaded preprocessed dataset: (36, 61)\n",
      "\n",
      " Selected 5 clustering features:\n",
      "  1. Jan25_Op_Rate\n",
      "  2. RuPay_Penetration\n",
      "  3. Rural_Percent\n",
      "  4. Avg_Balance_Rs\n",
      "  5. CAGR_2020_25\n",
      "\n",
      " Clustering data shape: (36, 5)\n",
      " Number of states: 36\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 1: DATA LOADING AND FEATURE SELECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load the preprocessed full dataset\n",
    "df = pd.read_csv('ml_preprocessed_full.csv')\n",
    "print(f\" Loaded preprocessed dataset: {df.shape}\")\n",
    "\n",
    "# Define clustering features based on methodology\n",
    "clustering_features = [\n",
    "    'Jan25_Op_Rate',           # Operative Account Rate (%)\n",
    "    'RuPay_Penetration',       # RuPay Card Penetration (%)\n",
    "    'Rural_Percent',           # Rural Dominance Index\n",
    "    'Avg_Balance_Rs',          # Average Balance per Account (₹)\n",
    "    'CAGR_2020_25'            # Account Growth Rate 2020-2025 (%)\n",
    "]\n",
    "\n",
    "# Check for availability of features\n",
    "available_features = []\n",
    "missing_features = []\n",
    "\n",
    "for feature in clustering_features:\n",
    "    if feature in df.columns:\n",
    "        available_features.append(feature)\n",
    "    else:\n",
    "        missing_features.append(feature)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n Missing features: {missing_features}\")\n",
    "    print(\"Attempting to find alternative column names...\")\n",
    "    \n",
    "    # Try alternative names\n",
    "    alternatives = {\n",
    "        'RuPay_Penetration': ['Jul25_RuPay_Penetration', 'RuPay_Cards', 'High_RuPay_Flag'],\n",
    "        'Avg_Balance_Rs': ['Jul25_Avg_Balance', 'Balance_Rupees', 'High_Balance_Flag']\n",
    "    }\n",
    "    \n",
    "    for missing in missing_features:\n",
    "        if missing in alternatives:\n",
    "            for alt in alternatives[missing]:\n",
    "                if alt in df.columns:\n",
    "                    available_features.append(alt)\n",
    "                    print(f\"  → Using '{alt}' instead of '{missing}'\")\n",
    "                    break\n",
    "\n",
    "# Update clustering features list\n",
    "clustering_features = available_features\n",
    "\n",
    "print(f\"\\n Selected {len(clustering_features)} clustering features:\")\n",
    "for i, feature in enumerate(clustering_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "# Extract state names for labeling\n",
    "if 'State_Name_Std' in df.columns:\n",
    "    state_names = df['State_Name_Std'].values\n",
    "elif 'State/UT' in df.columns:\n",
    "    state_names = df['State/UT'].values\n",
    "else:\n",
    "    state_names = [f\"State_{i}\" for i in range(len(df))]\n",
    "\n",
    "# Prepare clustering data\n",
    "X = df[clustering_features].copy()\n",
    "\n",
    "print(f\"\\n Clustering data shape: {X.shape}\")\n",
    "print(f\" Number of states: {len(state_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc40760-d053-4056-815b-468ab4029bc3",
   "metadata": {},
   "source": [
    "### STEP 2: DATA PREPROCESSING FOR CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f53f81e6-3c4c-40b5-9477-48af4131fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: DATA PREPROCESSING FOR CLUSTERING\n",
      "--------------------------------------------------\n",
      "\n",
      "Handling outliers using IQR method:\n",
      " Capped 11 outlier values\n",
      "\n",
      "Standardizing features:\n",
      " Features standardized using Z-score normalization\n",
      "\n",
      "Feature statistics after scaling:\n",
      "       Jan25_Op_Rate  RuPay_Penetration  Rural_Percent  Avg_Balance_Rs  \\\n",
      "count         36.000             36.000         36.000          36.000   \n",
      "mean           0.000             -0.000          0.000           0.000   \n",
      "std            1.014              1.014          1.014           1.014   \n",
      "min           -2.165             -2.464         -2.423          -1.769   \n",
      "25%           -0.500             -0.544         -0.575          -0.418   \n",
      "50%            0.108              0.113          0.027          -0.054   \n",
      "75%            0.610              0.737          0.656           0.482   \n",
      "max            1.966              1.509          1.918           1.833   \n",
      "\n",
      "       CAGR_2020_25  \n",
      "count        36.000  \n",
      "mean          0.000  \n",
      "std           1.014  \n",
      "min          -2.410  \n",
      "25%          -0.548  \n",
      "50%          -0.043  \n",
      "75%           0.693  \n",
      "max           2.053  \n",
      "STEP 3: DETERMINING OPTIMAL NUMBER OF CLUSTERS\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating different k values:\n",
      "----------------------------------------\n",
      "k=2: Silhouette=0.198, Davies-Bouldin=1.658\n",
      "k=3: Silhouette=0.218, Davies-Bouldin=1.374\n",
      "k=4: Silhouette=0.223, Davies-Bouldin=1.316\n",
      "k=5: Silhouette=0.206, Davies-Bouldin=1.248\n",
      "k=6: Silhouette=0.210, Davies-Bouldin=1.197\n",
      "k=7: Silhouette=0.227, Davies-Bouldin=1.098\n",
      "\n",
      "----------------------------------------\n",
      "Gap Statistic Analysis for k=3:\n",
      "Gap(k=3) = 0.349 ± 0.084\n",
      "Gap(k=4) = 0.368 ± 0.083\n",
      " Gap statistic supports k=3\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: DATA PREPROCESSING FOR CLUSTERING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = X.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nHandling missing values:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    X = X.fillna(X.median())\n",
    "    print(\" Missing values imputed with median\")\n",
    "\n",
    "# Handle outliers using IQR method\n",
    "print(\"\\nHandling outliers using IQR method:\")\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Cap outliers at 1.5×IQR\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_count = 0\n",
    "for col in X.columns:\n",
    "    outliers_mask = (X[col] < lower_bound[col]) | (X[col] > upper_bound[col])\n",
    "    outliers_count += outliers_mask.sum()\n",
    "    X.loc[X[col] < lower_bound[col], col] = lower_bound[col]\n",
    "    X.loc[X[col] > upper_bound[col], col] = upper_bound[col]\n",
    "\n",
    "print(f\" Capped {outliers_count} outlier values\")\n",
    "\n",
    "# Standardize features (Z-score normalization)\n",
    "print(\"\\nStandardizing features:\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=clustering_features)\n",
    "\n",
    "print(\" Features standardized using Z-score normalization\")\n",
    "print(\"\\nFeature statistics after scaling:\")\n",
    "print(X_scaled_df.describe().round(3))\n",
    "\n",
    "### STEP 3: OPTIMAL K DETERMINATION\n",
    "\n",
    "print(\"STEP 3: DETERMINING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test different values of k\n",
    "k_range = range(2, 8)\n",
    "metrics = {\n",
    "    'k': [],\n",
    "    'inertia': [],\n",
    "    'silhouette': [],\n",
    "    'calinski': [],\n",
    "    'davies_bouldin': []\n",
    "}\n",
    "\n",
    "print(\"\\nEvaluating different k values:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=50, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    metrics['k'].append(k)\n",
    "    metrics['inertia'].append(kmeans.inertia_)\n",
    "    metrics['silhouette'].append(silhouette_score(X_scaled, labels))\n",
    "    metrics['calinski'].append(calinski_harabasz_score(X_scaled, labels))\n",
    "    metrics['davies_bouldin'].append(davies_bouldin_score(X_scaled, labels))\n",
    "    \n",
    "    print(f\"k={k}: Silhouette={metrics['silhouette'][-1]:.3f}, \"\n",
    "          f\"Davies-Bouldin={metrics['davies_bouldin'][-1]:.3f}\")\n",
    "\n",
    "# Gap Statistic for k=3 validation\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Gap Statistic Analysis for k=3:\")\n",
    "\n",
    "def calculate_gap_statistic(X, k, n_refs=100):\n",
    "    \"\"\"Calculate gap statistic for given k\"\"\"\n",
    "    kmeans = KMeans(n_clusters=k, n_init=50, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    # Observed within-cluster sum of squares\n",
    "    obs_inertia = kmeans.inertia_\n",
    "    \n",
    "    # Generate reference datasets\n",
    "    ref_inertias = []\n",
    "    for _ in range(n_refs):\n",
    "        # Random data with same shape and bounds\n",
    "        random_data = np.random.uniform(X.min(axis=0), X.max(axis=0), X.shape)\n",
    "        kmeans_ref = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        kmeans_ref.fit(random_data)\n",
    "        ref_inertias.append(kmeans_ref.inertia_)\n",
    "    \n",
    "    # Calculate gap\n",
    "    gap = np.log(np.mean(ref_inertias)) - np.log(obs_inertia)\n",
    "    sdk = np.std(np.log(ref_inertias)) * np.sqrt(1 + 1/n_refs)\n",
    "    \n",
    "    return gap, sdk\n",
    "\n",
    "gap_3, sdk_3 = calculate_gap_statistic(X_scaled, k=3, n_refs=100)\n",
    "gap_4, sdk_4 = calculate_gap_statistic(X_scaled, k=4, n_refs=100)\n",
    "\n",
    "print(f\"Gap(k=3) = {gap_3:.3f} ± {sdk_3:.3f}\")\n",
    "print(f\"Gap(k=4) = {gap_4:.3f} ± {sdk_4:.3f}\")\n",
    "\n",
    "if gap_3 > gap_4 - sdk_4:\n",
    "    print(\" Gap statistic supports k=3\")\n",
    "else:\n",
    "    print(\" Gap statistic suggests k>3 might be better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75fea59-34f4-431e-b8b4-25c4cb8ffbc6",
   "metadata": {},
   "source": [
    "### STEP 4: K-MEANS CLUSTERING WITH K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a3f22ab-e370-4cc6-8e50-6bfa08a20d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: K-MEANS CLUSTERING (K=3)\n",
      "--------------------------------------------------\n",
      "\n",
      "Performing K-means with 100 random initializations...\n",
      " Best silhouette score: 0.218\n",
      "\n",
      "Cluster Distribution:\n",
      "------------------------------\n",
      "Cluster 0: 15 states (41.7%)\n",
      "Cluster 1: 15 states (41.7%)\n",
      "Cluster 2: 6 states (16.7%)\n",
      "\n",
      " Warning: Smallest cluster has only 6 states\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 4: K-MEANS CLUSTERING (K=3)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Perform K-means with k=3 and multiple initializations\n",
    "print(\"\\nPerforming K-means with 100 random initializations...\")\n",
    "\n",
    "best_silhouette = -1\n",
    "best_kmeans = None\n",
    "best_labels = None\n",
    "\n",
    "for i in range(100):\n",
    "    kmeans = KMeans(n_clusters=3, n_init=1, max_iter=300, \n",
    "                    tol=1e-4, random_state=i)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    \n",
    "    if silhouette > best_silhouette:\n",
    "        best_silhouette = silhouette\n",
    "        best_kmeans = kmeans\n",
    "        best_labels = labels\n",
    "\n",
    "print(f\" Best silhouette score: {best_silhouette:.3f}\")\n",
    "\n",
    "# Cluster statistics\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(\"-\" * 30)\n",
    "unique, counts = np.unique(best_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"Cluster {cluster}: {count} states ({count/len(best_labels)*100:.1f}%)\")\n",
    "\n",
    "# Check minimum cluster size\n",
    "min_cluster_size = min(counts)\n",
    "if min_cluster_size < 10:\n",
    "    print(f\"\\n Warning: Smallest cluster has only {min_cluster_size} states\")\n",
    "else:\n",
    "    print(f\"\\n All clusters have adequate size (min: {min_cluster_size} states)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1767abed-014c-42ec-b72b-1050acdd04cb",
   "metadata": {},
   "source": [
    "### STEP 5: CLUSTER STABILITY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9514e983-45b5-4ba7-aa48-9469d0023a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: CLUSTER STABILITY ANALYSIS\n",
      "--------------------------------------------------\n",
      "\n",
      "Performing bootstrap stability analysis (1000 iterations)...\n",
      "\n",
      "Cluster Stability (Jaccard similarity):\n",
      "  Cluster 0: 0.682\n",
      "  Cluster 1: 0.738\n",
      "  Cluster 2: 0.797\n",
      "\n",
      "Overall stability: 0.739\n",
      " Clusters show good stability\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 5: CLUSTER STABILITY ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nPerforming bootstrap stability analysis (1000 iterations)...\")\n",
    "\n",
    "n_bootstrap = 1000\n",
    "n_states = len(X_scaled)\n",
    "stability_matrix = np.zeros((n_states, n_states))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Bootstrap sample\n",
    "    idx = np.random.choice(n_states, n_states, replace=True)\n",
    "    X_boot = X_scaled[idx]\n",
    "    \n",
    "    # Cluster bootstrap sample\n",
    "    kmeans_boot = KMeans(n_clusters=3, n_init=10, random_state=i)\n",
    "    labels_boot = kmeans_boot.fit_predict(X_boot)\n",
    "    \n",
    "    # Update co-occurrence matrix\n",
    "    for j in range(n_states):\n",
    "        for k in range(j+1, n_states):\n",
    "            if idx[j] == idx[k]:  # Same original state\n",
    "                continue\n",
    "            if labels_boot[j] == labels_boot[k]:\n",
    "                stability_matrix[idx[j], idx[k]] += 1\n",
    "                stability_matrix[idx[k], idx[j]] += 1\n",
    "\n",
    "# Normalize by number of co-occurrences\n",
    "stability_matrix = stability_matrix / n_bootstrap\n",
    "\n",
    "# Calculate average stability for each cluster\n",
    "cluster_stability = []\n",
    "for cluster in range(3):\n",
    "    cluster_mask = best_labels == cluster\n",
    "    if cluster_mask.sum() > 1:\n",
    "        cluster_pairs = stability_matrix[np.ix_(cluster_mask, cluster_mask)]\n",
    "        np.fill_diagonal(cluster_pairs, 0)\n",
    "        avg_stability = cluster_pairs.sum() / (cluster_mask.sum() * (cluster_mask.sum() - 1))\n",
    "        cluster_stability.append(avg_stability)\n",
    "    else:\n",
    "        cluster_stability.append(0)\n",
    "\n",
    "print(\"\\nCluster Stability (Jaccard similarity):\")\n",
    "for i, stability in enumerate(cluster_stability):\n",
    "    print(f\"  Cluster {i}: {stability:.3f}\")\n",
    "\n",
    "overall_stability = np.mean(cluster_stability)\n",
    "print(f\"\\nOverall stability: {overall_stability:.3f}\")\n",
    "\n",
    "if overall_stability > 0.65:\n",
    "    print(\" Clusters show good stability\")\n",
    "else:\n",
    "    print(\" Clusters show moderate stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872e7ac-f223-4c5e-838e-3609cbce736e",
   "metadata": {},
   "source": [
    "### STEP 6: HIERARCHICAL CLUSTERING VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc5cbeeb-c2de-4295-a389-c44c1248238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: HIERARCHICAL CLUSTERING VALIDATION\n",
      "--------------------------------------------------\n",
      "\n",
      "Performing hierarchical clustering with Ward's method...\n",
      " Cophenetic correlation: 0.539\n",
      " Agreement between K-means and Hierarchical: 0.519\n",
      " Good agreement between clustering methods\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 6: HIERARCHICAL CLUSTERING VALIDATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "print(\"\\nPerforming hierarchical clustering with Ward's method...\")\n",
    "\n",
    "linkage_matrix = linkage(X_scaled, method='ward')\n",
    "hierarchical_labels = fcluster(linkage_matrix, 3, criterion='maxclust') - 1\n",
    "\n",
    "# Calculate cophenetic correlation\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "cophenetic_corr, cophenetic_distances = cophenet(linkage_matrix, pdist(X_scaled))\n",
    "print(f\" Cophenetic correlation: {cophenetic_corr:.3f}\")\n",
    "\n",
    "# Compare with K-means results\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "agreement = adjusted_rand_score(best_labels, hierarchical_labels)\n",
    "print(f\" Agreement between K-means and Hierarchical: {agreement:.3f}\")\n",
    "\n",
    "if agreement > 0.5:\n",
    "    print(\" Good agreement between clustering methods\")\n",
    "else:\n",
    "    print(\" Moderate agreement between methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85388d5-da02-4782-8127-1cb930c963bd",
   "metadata": {},
   "source": [
    "### STEP 7: CLUSTER CHARACTERIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50b7823d-f727-47c5-8f62-757e0f2a21df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: CLUSTER CHARACTERIZATION\n",
      "--------------------------------------------------\n",
      "\n",
      "Cluster Centroids (Original Scale):\n",
      "--------------------------------------------------\n",
      "   Jan25_Op_Rate  RuPay_Penetration  Rural_Percent  Avg_Balance_Rs  \\\n",
      "0           0.70               0.56           0.80            0.37   \n",
      "1           0.58               0.79           0.54           -0.42   \n",
      "2           0.21               0.76           0.74            0.48   \n",
      "\n",
      "   CAGR_2020_25  \n",
      "0          0.49  \n",
      "1          0.11  \n",
      "2         -1.46  \n",
      "\n",
      "--------------------------------------------------\n",
      "Statistical Significance of Feature Differences:\n",
      "--------------------------------------------------\n",
      "\n",
      "Jan25_Op_Rate:\n",
      "  ANOVA: F=25.89, p=0.0000\n",
      "  Kruskal-Wallis: H=17.40, p=0.0002\n",
      "   Significant difference between clusters\n",
      "\n",
      "RuPay_Penetration:\n",
      "  ANOVA: F=6.24, p=0.0050\n",
      "  Kruskal-Wallis: H=9.16, p=0.0102\n",
      "   Significant difference between clusters\n",
      "\n",
      "Rural_Percent:\n",
      "  ANOVA: F=19.68, p=0.0000\n",
      "  Kruskal-Wallis: H=21.50, p=0.0000\n",
      "   Significant difference between clusters\n",
      "\n",
      "Avg_Balance_Rs:\n",
      "  ANOVA: F=2.53, p=0.0952\n",
      "  Kruskal-Wallis: H=4.49, p=0.1059\n",
      "   No significant difference\n",
      "\n",
      "CAGR_2020_25:\n",
      "  ANOVA: F=14.99, p=0.0000\n",
      "  Kruskal-Wallis: H=12.87, p=0.0016\n",
      "   Significant difference between clusters\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 7: CLUSTER CHARACTERIZATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate cluster centroids\n",
    "cluster_centers = pd.DataFrame(\n",
    "    best_kmeans.cluster_centers_,\n",
    "    columns=clustering_features\n",
    ")\n",
    "\n",
    "# Transform back to original scale for interpretation\n",
    "cluster_centers_original = pd.DataFrame(\n",
    "    scaler.inverse_transform(cluster_centers),\n",
    "    columns=clustering_features\n",
    ")\n",
    "\n",
    "print(\"\\nCluster Centroids (Original Scale):\")\n",
    "print(\"-\" * 50)\n",
    "print(cluster_centers_original.round(2))\n",
    "\n",
    "# Statistical tests for feature differences\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"Statistical Significance of Feature Differences:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for feature in clustering_features:\n",
    "    groups = [X[feature].values[best_labels == i] for i in range(3)]\n",
    "    \n",
    "    # ANOVA for normally distributed features\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    \n",
    "    # Kruskal-Wallis for non-parametric alternative\n",
    "    h_stat, p_value_kw = kruskal(*groups)\n",
    "    \n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  ANOVA: F={f_stat:.2f}, p={p_value:.4f}\")\n",
    "    print(f\"  Kruskal-Wallis: H={h_stat:.2f}, p={p_value_kw:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"   Significant difference between clusters\")\n",
    "    else:\n",
    "        print(f\"   No significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7a790-13e4-4bfc-ad70-6e9bf9b4b11e",
   "metadata": {},
   "source": [
    "### STEP 8: CLUSTER PROFILING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "282694b8-8c10-4723-b3c8-fde2beed5fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 8: CLUSTER PROFILING\n",
      "--------------------------------------------------\n",
      "\n",
      "Cluster Profiles:\n",
      "==================================================\n",
      "\n",
      "Cluster 0: Emerging States\n",
      "  Number of states: 15\n",
      "  Sample states: ANDAMAN AND NICOBAR ISLANDS, ARUNACHAL PRADESH, ASSAM, CHHATTISGARH, HIMACHAL PRADESH...\n",
      "  Key Characteristics:\n",
      "    Jan25_Op_Rate: 0.70 (±0.15)\n",
      "    RuPay_Penetration: 0.56 (±0.21)\n",
      "    Rural_Percent: 0.80 (±0.11)\n",
      "    Avg_Balance_Rs: 0.37 (±1.02)\n",
      "    CAGR_2020_25: 0.49 (±0.85)\n",
      "\n",
      "Cluster 1: Development States\n",
      "  Number of states: 15\n",
      "  Sample states: ANDHRA PRADESH, BIHAR, CHANDIGARH, DELHI, GUJARAT...\n",
      "  Key Characteristics:\n",
      "    Jan25_Op_Rate: 0.58 (±0.12)\n",
      "    RuPay_Penetration: 0.79 (±0.15)\n",
      "    Rural_Percent: 0.54 (±0.13)\n",
      "    Avg_Balance_Rs: -0.42 (±0.87)\n",
      "    CAGR_2020_25: 0.11 (±0.51)\n",
      "\n",
      "Cluster 2: Priority Intervention States\n",
      "  Number of states: 6\n",
      "  Sample states: DAMAN & DIU, GOA, JAMMU & KASHMIR, LADAKH, MANIPUR...\n",
      "  Key Characteristics:\n",
      "    Jan25_Op_Rate: 0.21 (±0.12)\n",
      "    RuPay_Penetration: 0.76 (±0.10)\n",
      "    Rural_Percent: 0.74 (±0.08)\n",
      "    Avg_Balance_Rs: 0.48 (±1.38)\n",
      "    CAGR_2020_25: -1.46 (±0.76)\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 8: CLUSTER PROFILING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create detailed cluster profiles\n",
    "cluster_profiles = []\n",
    "\n",
    "for cluster_id in range(3):\n",
    "    cluster_mask = best_labels == cluster_id\n",
    "    cluster_states = state_names[cluster_mask]\n",
    "    \n",
    "    profile = {\n",
    "        'cluster_id': cluster_id,\n",
    "        'n_states': cluster_mask.sum(),\n",
    "        'states': ', '.join(cluster_states[:5]) + ('...' if len(cluster_states) > 5 else ''),\n",
    "        'characteristics': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate mean values for each feature\n",
    "    for feature in clustering_features:\n",
    "        profile['characteristics'][feature] = {\n",
    "            'mean': X[feature].values[cluster_mask].mean(),\n",
    "            'std': X[feature].values[cluster_mask].std()\n",
    "        }\n",
    "    \n",
    "    cluster_profiles.append(profile)\n",
    "\n",
    "# Assign cluster names based on characteristics\n",
    "cluster_names = []\n",
    "for i, profile in enumerate(cluster_profiles):\n",
    "    chars = profile['characteristics']\n",
    "    \n",
    "    # Determine cluster type based on key metrics\n",
    "    op_rate = chars.get('Jan25_Op_Rate', {}).get('mean', 0)\n",
    "    growth = chars.get('CAGR_2020_25', {}).get('mean', 0)\n",
    "    \n",
    "    if i == 0:  # Customize based on actual characteristics\n",
    "        if op_rate > 80:\n",
    "            cluster_names.append(\"High Performers\")\n",
    "        elif growth > 10:\n",
    "            cluster_names.append(\"Growth Markets\")\n",
    "        else:\n",
    "            cluster_names.append(\"Emerging States\")\n",
    "    elif i == 1:\n",
    "        if op_rate > 70 and op_rate <= 80:\n",
    "            cluster_names.append(\"Moderate Performers\")\n",
    "        else:\n",
    "            cluster_names.append(\"Development States\")\n",
    "    else:\n",
    "        cluster_names.append(\"Priority Intervention States\")\n",
    "\n",
    "print(\"\\nCluster Profiles:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, (profile, name) in enumerate(zip(cluster_profiles, cluster_names)):\n",
    "    print(f\"\\nCluster {i}: {name}\")\n",
    "    print(f\"  Number of states: {profile['n_states']}\")\n",
    "    print(f\"  Sample states: {profile['states']}\")\n",
    "    print(f\"  Key Characteristics:\")\n",
    "    \n",
    "    for feature, stats in profile['characteristics'].items():\n",
    "        print(f\"    {feature}: {stats['mean']:.2f} (±{stats['std']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f173494-9626-4c0a-b66e-6faa1eafc40e",
   "metadata": {},
   "source": [
    "### STEP 9: ALTERNATIVE CLUSTERING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0320cffe-9677-4da4-a70f-18d171f1da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 9: VALIDATION WITH ALTERNATIVE METHODS\n",
      "--------------------------------------------------\n",
      "\n",
      "Gaussian Mixture Model (k=3):\n",
      "  Silhouette score: 0.135\n",
      "  Agreement with K-means: 0.488\n",
      "\n",
      "DBSCAN (density-based clustering):\n",
      "  Number of clusters: 1\n",
      "  Noise points: 0\n",
      "   DBSCAN found only 1 cluster - parameters may need adjustment\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 9: VALIDATION WITH ALTERNATIVE METHODS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Gaussian Mixture Model\n",
    "print(\"\\nGaussian Mixture Model (k=3):\")\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "gmm_silhouette = silhouette_score(X_scaled, gmm_labels)\n",
    "print(f\"  Silhouette score: {gmm_silhouette:.3f}\")\n",
    "print(f\"  Agreement with K-means: {adjusted_rand_score(best_labels, gmm_labels):.3f}\")\n",
    "\n",
    "# DBSCAN (density-based)\n",
    "print(\"\\nDBSCAN (density-based clustering):\")\n",
    "dbscan = DBSCAN(eps=2.5, min_samples=3)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "print(f\"  Number of clusters: {n_clusters_dbscan}\")\n",
    "print(f\"  Noise points: {n_noise}\")\n",
    "\n",
    "if n_clusters_dbscan > 1 and n_noise < len(dbscan_labels):\n",
    "    # Only calculate silhouette if we have more than 1 cluster\n",
    "    dbscan_silhouette = silhouette_score(X_scaled[dbscan_labels != -1], \n",
    "                                         dbscan_labels[dbscan_labels != -1])\n",
    "    print(f\"  Silhouette score: {dbscan_silhouette:.3f}\")\n",
    "elif n_clusters_dbscan == 1:\n",
    "    print(\"   DBSCAN found only 1 cluster - parameters may need adjustment\")\n",
    "else:\n",
    "    print(\"   DBSCAN could not find meaningful clusters with current parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9fe3b-2927-41ec-a9b3-019af3462b36",
   "metadata": {},
   "source": [
    "### STEP 10: FINAL RESULTS AND RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b468bf29-6395-4d5a-9968-b694befaaf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 10: FINAL RESULTS AND RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "CLUSTERING ANALYSIS SUMMARY\n",
      "==================================================\n",
      "\n",
      " Optimal number of clusters: 3\n",
      " Best silhouette score: 0.218\n",
      " Cluster stability: 0.739\n",
      " Method agreement: 0.519\n",
      "\n",
      "--------------------------------------------------\n",
      "FINAL CLUSTER ASSIGNMENTS:\n",
      "--------------------------------------------------\n",
      "\n",
      "Cluster Assignments by State:\n",
      "\n",
      "Emerging States (Cluster 0):\n",
      "  - ANDAMAN AND NICOBAR ISLANDS\n",
      "  - ARUNACHAL PRADESH\n",
      "  - ASSAM\n",
      "  - CHHATTISGARH\n",
      "  - HIMACHAL PRADESH\n",
      "  - JHARKHAND\n",
      "  - KERALA\n",
      "  - LAKSHADWEEP\n",
      "  - MEGHALAYA\n",
      "  - MIZORAM\n",
      "  - ODISHA\n",
      "  - TRIPURA\n",
      "  - UTTAR PRADESH\n",
      "  - UTTARAKHAND\n",
      "  - WEST BENGAL\n",
      "\n",
      "Development States (Cluster 1):\n",
      "  - ANDHRA PRADESH\n",
      "  - BIHAR\n",
      "  - CHANDIGARH\n",
      "  - DELHI\n",
      "  - GUJARAT\n",
      "  - HARYANA\n",
      "  - KARNATAKA\n",
      "  - MADHYA PRADESH\n",
      "  - MAHARASHTRA\n",
      "  - NAGALAND\n",
      "  - PUDUCHERRY\n",
      "  - PUNJAB\n",
      "  - RAJASTHAN\n",
      "  - TAMIL NADU\n",
      "  - TELANGANA\n",
      "\n",
      "Priority Intervention States (Cluster 2):\n",
      "  - DAMAN & DIU\n",
      "  - GOA\n",
      "  - JAMMU & KASHMIR\n",
      "  - LADAKH\n",
      "  - MANIPUR\n",
      "  - SIKKIM\n",
      "\n",
      " Results saved to 'rq3_cluster_results.csv'\n",
      "\n",
      "================================================================================\n",
      "POLICY RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Emerging States (Cluster 0):\n",
      "  → Priority intervention required\n",
      "  → Intensive support for account activation\n",
      "  → Financial literacy campaigns needed\n",
      "\n",
      "Development States (Cluster 1):\n",
      "  → Priority intervention required\n",
      "  → Intensive support for account activation\n",
      "  → Financial literacy campaigns needed\n",
      "\n",
      "Priority Intervention States (Cluster 2):\n",
      "  → Priority intervention required\n",
      "  → Intensive support for account activation\n",
      "  → Financial literacy campaigns needed\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 10: FINAL RESULTS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CLUSTERING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n Optimal number of clusters: 3\")\n",
    "print(f\" Best silhouette score: {best_silhouette:.3f}\")\n",
    "print(f\" Cluster stability: {overall_stability:.3f}\")\n",
    "print(f\" Method agreement: {agreement:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"FINAL CLUSTER ASSIGNMENTS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create final results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'State': state_names,\n",
    "    'Cluster': best_labels,\n",
    "    'Cluster_Name': [cluster_names[label] for label in best_labels]\n",
    "})\n",
    "\n",
    "# Add original features for reference\n",
    "for feature in clustering_features:\n",
    "    results_df[feature] = X[feature].values\n",
    "\n",
    "# Sort by cluster\n",
    "results_df = results_df.sort_values(['Cluster', 'State'])\n",
    "\n",
    "print(\"\\nCluster Assignments by State:\")\n",
    "for cluster in range(3):\n",
    "    cluster_data = results_df[results_df['Cluster'] == cluster]\n",
    "    print(f\"\\n{cluster_names[cluster]} (Cluster {cluster}):\")\n",
    "    for _, row in cluster_data.iterrows():\n",
    "        print(f\"  - {row['State']}\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('rq3_cluster_results.csv', index=False)\n",
    "print(\"\\n Results saved to 'rq3_cluster_results.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"POLICY RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, name in enumerate(cluster_names):\n",
    "    cluster_mask = best_labels == i\n",
    "    print(f\"\\n{name} (Cluster {i}):\")\n",
    "    \n",
    "    # Get cluster characteristics\n",
    "    op_rate = X['Jan25_Op_Rate'].values[cluster_mask].mean() if 'Jan25_Op_Rate' in X.columns else 0\n",
    "    growth = X['CAGR_2020_25'].values[cluster_mask].mean() if 'CAGR_2020_25' in X.columns else 0\n",
    "    rural = X['Rural_Percent'].values[cluster_mask].mean() if 'Rural_Percent' in X.columns else 0\n",
    "    \n",
    "    if op_rate > 80:\n",
    "        print(\"  → Best practice states - document and share success strategies\")\n",
    "        print(\"  → Focus on maintaining momentum and innovation\")\n",
    "    elif op_rate > 70:\n",
    "        print(\"  → Target for optimization programs\")\n",
    "        print(\"  → Focus on improving RuPay adoption and transaction frequency\")\n",
    "    else:\n",
    "        print(\"  → Priority intervention required\")\n",
    "        print(\"  → Intensive support for account activation\")\n",
    "        print(\"  → Financial literacy campaigns needed\")\n",
    "    \n",
    "    if rural > 70:\n",
    "        print(\"  → Rural-focused initiatives required\")\n",
    "        print(\"  → Mobile banking and BC network expansion\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
